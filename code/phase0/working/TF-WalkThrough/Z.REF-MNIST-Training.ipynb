{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49666abe",
   "metadata": {},
   "source": [
    "# MNIST: Custom training: walkthrough\n",
    "- https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b952b",
   "metadata": {},
   "source": [
    "# 0. 환경 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fbbcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d77541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.3\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b6027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[6], 'GPU')\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f351f7",
   "metadata": {},
   "source": [
    "# 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c8fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_images, mnist_labels), (x_test, y_test) = tf.keras.datasets.mnist.load_data(\n",
    "    path=\"mnist-%d.npz\" % 10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f635b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <class 'numpy.ndarray'>\n",
      "shape:  (60000, 28, 28)\n",
      "mnist_labels:  (60000,)\n",
      "x_test:  (10000, 28, 28)\n",
      "y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"type: \", type(mnist_images))\n",
    "print(\"shape: \", mnist_images.shape)\n",
    "print(\"mnist_labels: \", mnist_labels.shape)\n",
    "\n",
    "print(\"x_test: \", x_test.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a86dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset(mnist_images, mnist_labels, batch_size, buffer_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32), \n",
    "         tf.cast(mnist_labels, tf.int64))\n",
    "    )\n",
    "    dataset = dataset.repeat(1).shuffle(buffer_size).batch(batch_size)\n",
    "    print(\"# of batches : {0}\".format(tf.data.experimental.cardinality(dataset).numpy()))\n",
    "\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "def create_test_dataset(mnist_images, mnist_labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32), \n",
    "         tf.cast(mnist_labels, tf.int64))\n",
    "    )\n",
    "    dataset = dataset.repeat(1).batch(batch_size)\n",
    "    print(\"# of batches : {0}\".format(tf.data.experimental.cardinality(dataset).numpy()))\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205d4124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer_size:  60000\n",
      "# of batches : 235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "buffer_size = mnist_images.shape[0]\n",
    "print(\"buffer_size: \", buffer_size)\n",
    "dataset = create_train_dataset(mnist_images, mnist_labels,batch_size, buffer_size)\n",
    "    \n",
    "# dataset = dataset.repeat(1).shuffle(buffer_size).batch(batch_size)\n",
    "# print(tf.data.experimental.cardinality(dataset))\n",
    "# print(tf.data.experimental.cardinality(dataset).numpy())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a03efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches : 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = x_test.shape[0]\n",
    "test_ds = create_test_dataset(x_test, y_test,batch_size)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4f4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features, labels = next(iter(dataset))\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a8a8a",
   "metadata": {},
   "source": [
    "# 2. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f826f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(32, [3, 3], activation=\"relu\"),\n",
    "        tf.keras.layers.Conv2D(64, [3, 3], activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb820f63",
   "metadata": {},
   "source": [
    "# 3. 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ebcc8",
   "metadata": {},
   "source": [
    "## Loss 오브젝트 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab05057",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe70c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(0.000125 * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efafd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(model=mnist_model, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af38bc",
   "metadata": {},
   "source": [
    "## Gradient 생성 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b5efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(images, labels, first_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = mnist_model(images, training=True)\n",
    "        print(\"probs: \", probs)\n",
    "        print(\"labels: \", labels)        \n",
    "        \n",
    "        loss_value = loss(labels, probs)\n",
    "        \n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "    \n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a04a01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = mnist_model(images, training=False)\n",
    "#   print(\"labels: \", labels)\n",
    "#   print(\"predictions: \", predictions)    \n",
    "    \n",
    "    t_loss = loss(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    return t_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f87c55",
   "metadata": {},
   "source": [
    "# 4. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9452b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-10-03 07:56:29.180 ip-172-16-67-78:124063 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-10-03 07:56:29.479 ip-172-16-67-78:124063 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "probs:  Tensor(\"sequential/dense_1/Softmax:0\", shape=(256, 10), dtype=float32)\n",
      "labels:  Tensor(\"labels:0\", shape=(256,), dtype=int64)\n",
      "probs:  Tensor(\"sequential/dense_1/Softmax:0\", shape=(256, 10), dtype=float32)\n",
      "labels:  Tensor(\"labels:0\", shape=(256,), dtype=int64)\n",
      "Epoch: 0\t, Step #0\tLoss: 2.312272\n",
      "probs:  Tensor(\"sequential/dense_1/Softmax:0\", shape=(256, 10), dtype=float32)\n",
      "labels:  Tensor(\"labels:0\", shape=(256,), dtype=int64)\n",
      "Epoch: 0\t, Step #100\tLoss: 0.235716\n",
      "Epoch: 0\t, Step #200\tLoss: 0.083622\n",
      "probs:  Tensor(\"sequential/dense_1/Softmax:0\", shape=(96, 10), dtype=float32)\n",
      "labels:  Tensor(\"labels:0\", shape=(96,), dtype=int64)\n",
      "Test Loss: 0.062250152230262756, Test Accuracy: 97.93999481201172\n",
      "Epoch: 1\t, Step #0\tLoss: 0.082664\n",
      "Epoch: 1\t, Step #100\tLoss: 0.127181\n",
      "Epoch: 1\t, Step #200\tLoss: 0.086330\n",
      "Test Loss: 0.0458381287753582, Test Accuracy: 98.50999450683594\n",
      "Epoch: 2\t, Step #0\tLoss: 0.084991\n",
      "Epoch: 2\t, Step #100\tLoss: 0.055204\n",
      "Epoch: 2\t, Step #200\tLoss: 0.026126\n",
      "Test Loss: 0.03514379635453224, Test Accuracy: 98.83000183105469\n",
      "Epoch: 3\t, Step #0\tLoss: 0.066537\n",
      "Epoch: 3\t, Step #100\tLoss: 0.055931\n",
      "Epoch: 3\t, Step #200\tLoss: 0.069662\n",
      "Test Loss: 0.034831613302230835, Test Accuracy: 98.7699966430664\n",
      "Epoch: 4\t, Step #0\tLoss: 0.031178\n",
      "Epoch: 4\t, Step #100\tLoss: 0.033881\n",
      "Epoch: 4\t, Step #200\tLoss: 0.076009\n",
      "Test Loss: 0.03226584568619728, Test Accuracy: 98.94000244140625\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "print_interval = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch, (images, labels) in enumerate(dataset):\n",
    "        loss_value = training_step(images, labels, batch == 0)\n",
    "\n",
    "        if batch % print_interval == 0:\n",
    "            print(\"Epoch: %d\\t, Step #%d\\tLoss: %.6f\" %  (epoch, batch, loss_value))\n",
    "            \n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "        print(\n",
    "        #ㅁ    f'Epoch {epoch + 1}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "        )\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5870b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for epoch in range(EPOCHS):\n",
    "# n_batch = 1000\n",
    "# print_interval = 1000\n",
    "# for batch, (images, labels) in enumerate(dataset.take(n_batch)):\n",
    "#     loss_value = training_step(images, labels, batch == 0)\n",
    "\n",
    "#     if batch % print_interval == 0:\n",
    "#         print(\"Step #%d\\tLoss: %.6f\" %  (batch, loss_value))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "331e5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "checkpoint_dir = log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f05bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: log/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: log/1/assets\n"
     ]
    }
   ],
   "source": [
    "mnist_model.save(os.path.join(checkpoint_dir, \"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa79ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b4785",
   "metadata": {},
   "source": [
    "# 5. 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d94cd668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.03226584568619728, Test Accuracy: 98.94000244140625\n"
     ]
    }
   ],
   "source": [
    "# Reset the metrics at the start of the next epoch\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "\n",
    "\n",
    "for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "    print(\n",
    "    #ㅁ    f'Epoch {epoch + 1}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa022bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
