{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 2.2] TF 호로보드로 분산 훈련 (로컬 모드 및 호스트 모드)\n",
    "\n",
    "\n",
    "본 워크샵의 모든 노트북은 **<font color=\"red\">conda_tensorflow2_p36</font>** 를 사용합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 1. 기본 환경 세팅 \n",
    "- 2. 데이터 세트를 S3 에 업로딩\n",
    "- 3. 노트북에서 세이지 메이커 스크립트 모드 스타일로 코드 변경\n",
    "- 4. 세이지 메이커 로컬 모드로 훈련\n",
    "- 5. 세이지 메이커의 호스트 모드로 훈련\n",
    "- 6. 모델 아티펙트 경로 저장\n",
    "\n",
    "\n",
    "## 참고:\n",
    "- 호로보드 깃의 TF2 공식 예시 입니다.\n",
    "    - [호로보드 공식 예제](https://github.com/horovod/horovod/blob/master/examples/tensorflow2/tensorflow2_mnist.py)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cnn-cifar10\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local_gpu\"\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_dir\n",
    "%store -r validation_dir\n",
    "%store -r eval_dir\n",
    "%store -r data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 세트를 S3에 업로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-227612457811/data/DEMO-cifar10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_location = sagemaker_session.upload_data(path=data_dir, key_prefix='data/DEMO-cifar10')\n",
    "display(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 훈련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local_gpu\"\n",
    "# instance_type = \"ml.p3.8xlarge\"\n",
    "\n",
    "job_name ='cifar10-horovod'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시스템의 이전 도커 컨테이너 삭제\n",
    "- 아래와 같은 명령어를 사용하여 저장 공간을 확보 합니다.\n",
    "- 필요시 주석을 제거하고 사용하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도커 컨테이너 모두 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs         30G   72K   30G   1% /dev\n",
      "tmpfs            30G     0   30G   0% /dev/shm\n",
      "/dev/xvda1      109G   98G   11G  91% /\n",
      "/dev/xvdf        20G  571M   18G   4% /home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "! df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! docker container prune -f\n",
    "# ! df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도커 이미지 모두 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! df -h\n",
    "# ! docker image prune -f --all\n",
    "# ! df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 로컬모드로 훈련 \n",
    "- 현 실행 노트북 인스턴스에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_gpu_learning_rate:  0.001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count ):\n",
    "    total_gpu = num_gpu * train_instance_count\n",
    "\n",
    "    multi_gpu_learning_rate = one_gpu_learning_rate / total_gpu\n",
    "    print(\"multi_gpu_learning_rate: \", multi_gpu_learning_rate)\n",
    "    \n",
    "    return multi_gpu_learning_rate\n",
    "\n",
    "train_instance_type = 'local_gpu'\n",
    "num_gpu = 1\n",
    "train_instance_count = 1\n",
    "one_gpu_learning_rate = 0.001 \n",
    "\n",
    "multi_gpu_learning_rate = calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributions has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating xmn4yiyyrw-algo-1-twv56 ... \n",
      "Creating xmn4yiyyrw-algo-1-twv56 ... done\n",
      "Attaching to xmn4yiyyrw-algo-1-twv56\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:57.827877: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:57.828070: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:57.832327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:57.869090: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:59,598 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:59,848 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:59,849 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:59,855 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:59,855 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1-twv56'] Hosts: ['algo-1-twv56'] process_per_hosts: 1 num_processes: 1\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:59,857 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:36:59,886 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m Training Env:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m {\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"additional_framework_parameters\": {\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"sagemaker_mpi_enabled\": true,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"sagemaker_mpi_num_of_processes_per_host\": 1,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"sagemaker_mpi_custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\"\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     },\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"validation\": \"/opt/ml/input/data/validation\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"eval\": \"/opt/ml/input/data/eval\"\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     },\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"current_host\": \"algo-1-twv56\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"algo-1-twv56\"\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     ],\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"learning-rate\": 0.001,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"print-interval\": 100,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"train-batch-size\": 64,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"eval-batch-size\": 512,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"validation-batch-size\": 512,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"model_dir\": \"/opt/ml/model\"\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     },\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"train\": {\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         },\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"validation\": {\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         },\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"eval\": {\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         }\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     },\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"job_name\": \"cifar10-tf-dist-2021-10-11-11-36-53-513\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"master_hostname\": \"algo-1-twv56\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-227612457811/cifar10-tf-dist-2021-10-11-11-36-53-513/source/sourcedir.tar.gz\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"module_name\": \"cifar10_tf2_sm_horovod\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"num_cpus\": 8,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"num_gpus\": 1,\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"current_host\": \"algo-1-twv56\",\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m             \"algo-1-twv56\"\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m         ]\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     },\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m     \"user_entry_point\": \"cifar10_tf2_sm_horovod.py\"\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m }\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m Environment variables:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_HOSTS=[\"algo-1-twv56\"]\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_HPS={\"epochs\":1,\"eval-batch-size\":512,\"learning-rate\":0.001,\"model_dir\":\"/opt/ml/model\",\"print-interval\":100,\"train-batch-size\":64,\"validation-batch-size\":512}\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_USER_ENTRY_POINT=cifar10_tf2_sm_horovod.py\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"-verbose --NCCL_DEBUG=INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":1}\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-twv56\",\"hosts\":[\"algo-1-twv56\"]}\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_INPUT_DATA_CONFIG={\"eval\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_CHANNELS=[\"eval\",\"train\",\"validation\"]\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_CURRENT_HOST=algo-1-twv56\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_MODULE_NAME=cifar10_tf2_sm_horovod\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_NUM_CPUS=8\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_NUM_GPUS=1\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-227612457811/cifar10-tf-dist-2021-10-11-11-36-53-513/source/sourcedir.tar.gz\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"-verbose --NCCL_DEBUG=INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":1},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1-twv56\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-twv56\"],\"hyperparameters\":{\"epochs\":1,\"eval-batch-size\":512,\"learning-rate\":0.001,\"model_dir\":\"/opt/ml/model\",\"print-interval\":100,\"train-batch-size\":64,\"validation-batch-size\":512},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-tf-dist-2021-10-11-11-36-53-513\",\"log_level\":20,\"master_hostname\":\"algo-1-twv56\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-227612457811/cifar10-tf-dist-2021-10-11-11-36-53-513/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_tf2_sm_horovod\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-twv56\",\"hosts\":[\"algo-1-twv56\"]},\"user_entry_point\":\"cifar10_tf2_sm_horovod.py\"}\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\",\"--eval-batch-size\",\"512\",\"--learning-rate\",\"0.001\",\"--model_dir\",\"/opt/ml/model\",\"--print-interval\",\"100\",\"--train-batch-size\",\"64\",\"--validation-batch-size\",\"512\"]\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_CHANNEL_EVAL=/opt/ml/input/data/eval\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_HP_LEARNING-RATE=0.001\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_HP_PRINT-INTERVAL=100\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_HP_TRAIN-BATCH-SIZE=64\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_HP_EVAL-BATCH-SIZE=512\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_HP_VALIDATION-BATCH-SIZE=512\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m SM_HP_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m mpirun --host algo-1-twv56 -np 1 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -verbose -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN -x SM_CHANNEL_VALIDATION -x SM_CHANNEL_EVAL -x SM_HP_EPOCHS -x SM_HP_LEARNING-RATE -x SM_HP_PRINT-INTERVAL -x SM_HP_TRAIN-BATCH-SIZE -x SM_HP_EVAL-BATCH-SIZE -x SM_HP_VALIDATION-BATCH-SIZE -x SM_HP_MODEL_DIR -x PYTHONPATH /usr/local/bin/python3.7 -m mpi4py cifar10_tf2_sm_horovod.py --epochs 1 --eval-batch-size 512 --learning-rate 0.001 --model_dir /opt/ml/model --print-interval 100 --train-batch-size 64 --validation-batch-size 512\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m  Data for JOB [3070,1] offset 0 Total slots allocated 1\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m  ========================   JOB MAP   ========================\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m  Data for node: 3726dfadbc76\tNum slots: 1\tMax slots: 0\tNum procs: 1\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m  \tProcess OMPI jobid: [3070,1] App: 0 Process rank: 0 Bound: N/A\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m  =============================================================\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:## args: \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>: [1,0]<stdout>:Namespace(epochs=1, eval='/opt/ml/input/data/eval', eval_batch_size=512, learning_rate=0.001, model_dir='/opt/ml/model', model_output_dir='/opt/ml/model', momentum=0.9, optimizer='adam', print_interval=100, train='/opt/ml/input/data/train', train_batch_size=64, validation='/opt/ml/input/data/validation', validation_batch_size=512, weight_decay=0.0002)\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:################# Loading Dataset ################\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:Channel Name: train\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:# of batches loading TFRecord : 10000\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:buffer_size:  10000\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:################# Loading Dataset ################\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:Channel Name: eval\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:# of batches loading TFRecord : 10000\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:################# Loading Dataset ################\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:Channel Name: validation\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:# of batches loading TFRecord : 10000\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:################# Prepare Dataset ################\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:# of batches in train:  156\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:# of batches in eval:  19\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:# of batches in validation: [1,0]<stdout>: 19\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:################# Start Training  ################\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:## num_train_batch on each GPU0 : 156 \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:[2021-10-11 11:37:09.889 algo-1-twv56:43 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:[2021-10-11 11:37:09.922 algo-1-twv56:43 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 44.428829\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #100\tLoss: 2.129145\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:## Epoch 1, Test Loss: 2.22176194190979, Test Accuracy: 15.902548789978027\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:Training Finished.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:################# Start Training  ################\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:################# Saving Model  ################\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stdout>:Model is saved in /opt/ml/model\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stderr>:2021-10-11 11:37:00.103604: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stderr>:2021-10-11 11:37:00.103785: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stderr>:2021-10-11 11:37:00.145004: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stderr>:2021-10-11 11:37:16.808532: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m [1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m \n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 |\u001b[0m 2021-10-11 11:37:18,514 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mxmn4yiyyrw-algo-1-twv56 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "hyperparameters = {\n",
    "                    'epochs' : 1,\n",
    "                    'learning-rate' : float(f\"{multi_gpu_learning_rate}\"),\n",
    "                    'print-interval' : 100,\n",
    "                    'train-batch-size': 64,    \n",
    "                    'eval-batch-size': 512,        \n",
    "                    'validation-batch-size': 512,\n",
    "                  }\n",
    "\n",
    "distributions = {\n",
    "    'mpi': {\n",
    "        'enabled': True, \n",
    "        'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO',\n",
    "        'processes_per_host': int(f\"{num_gpu}\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Change base_job_name to 'cifar10-dist' for console visibility\n",
    "estimator = TensorFlow(base_job_name='cifar10-tf-dist',\n",
    "                       entry_point='cifar10_tf2_sm_horovod.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       framework_version='2.4.1',\n",
    "                       py_version='py37',\n",
    "                       script_mode=True,                            \n",
    "                       hyperparameters= hyperparameters,\n",
    "                       train_instance_count=train_instance_count,   # 변경\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       distributions=distributions # 추가\n",
    "                      )\n",
    "\n",
    "\n",
    "estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬모드에서 도커 이미지 다운로드 된 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                         TAG                 IMAGE ID            CREATED             SIZE\n",
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training   2.4.1-gpu-py37      8467bc1c5070        5 months ago        8.91GB\n"
     ]
    }
   ],
   "source": [
    "! docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 호스트 모드로 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">`processes_per_host: 4` 는 한개의 EC2에서 사용할 수 있는 총 GPU 의 개수 입니다.</font>\n",
    "\n",
    "- 아래 ml.p3.8xlarge 는 4개의 GPU 가 있고, 최대 4개를 사용 할 수 있습니다.\n",
    "- 아래의 경우는 ml.p3.8xlarge 는 4개의 GPU 가 있고, 2개의 인스턴스이기에 종합적으로 8개를 사용 할 수 있습니다.\n",
    "    - train_instance_type='ml.p3.8xlarge'\n",
    "    - train_instance_count = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "train_instance_type='ml.p3.8xlarge'\n",
    "train_instance_count = 2\n",
    "\n",
    "distributions = {\n",
    "    'mpi': {\n",
    "        'enabled': True, \n",
    "        'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO',\n",
    "        'processes_per_host': 4\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## multi_gpu_learning_rate\n",
    "- GPU 의 개수, Batch Size, Epoch 당 배치 수 에 따라 튜닝이 필요한 수치 입니다. 여기서는 예시로 사용한 것이기에, 실제 사용시에 적절하게 튜닝을 해주시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_gpu_learning_rate:  0.000125\n"
     ]
    }
   ],
   "source": [
    "train_instance_type = 'ml.p3.8xlarge'\n",
    "num_gpu = 4\n",
    "train_instance_count = 2\n",
    "total_num_gpu = num_gpu * train_instance_count\n",
    "one_gpu_learning_rate = 0.001 \n",
    "\n",
    "multi_gpu_learning_rate = calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {'Name': 'train:loss', 'Regex': 'loss: (.*?) '},\n",
    "    {'Name': 'train:accuracy', 'Regex': 'acc: (.*?) '},\n",
    "    {'Name': 'validation:loss', 'Regex': 'val_loss: (.*?) '},\n",
    "    {'Name': 'validation:accuracy', 'Regex': 'val_acc: (.*?) '}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "                    'epochs' : 40,\n",
    "                    'learning-rate' : float(f\"{multi_gpu_learning_rate}\"),\n",
    "                    'print-interval' : 50,\n",
    "                    'train-batch-size': 64,    \n",
    "                    'eval-batch-size': 512,        \n",
    "                    'validation-batch-size': 512,\n",
    "                  }\n",
    "\n",
    "\n",
    "distributions = {\n",
    "    'mpi': {\n",
    "        'enabled': True, \n",
    "        'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO',\n",
    "        'processes_per_host': int(f\"{num_gpu}\")\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributions has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "\n",
    "horovod_estimator = TensorFlow(base_job_name='cifar10-tf-dist',\n",
    "                       entry_point='cifar10_tf2_sm_horovod.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       framework_version='2.4.1',\n",
    "                       py_version='py37',\n",
    "                       script_mode=True,                            \n",
    "                       hyperparameters= hyperparameters,\n",
    "                       train_instance_count=train_instance_count,   # 변경\n",
    "                       train_instance_type= train_instance_type,\n",
    "                       distributions=distributions # 추가\n",
    "                      )\n",
    "\n",
    "\n",
    "horovod_estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 11:37:52 Starting - Starting the training job...\n",
      "2021-10-11 11:38:21 Starting - Launching requested ML instancesProfilerReport-1633952272: InProgress\n",
      ".........\n",
      "2021-10-11 11:39:44 Starting - Preparing the instances for training......\n",
      "2021-10-11 11:40:43 Downloading - Downloading input data...\n",
      "2021-10-11 11:41:21 Training - Downloading the training image..............\u001b[34m2021-10-11 11:43:36.122570: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:36.127899: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:36.225247: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:36.328498: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:39,898 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:40,495 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:40,496 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:40,502 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:40,503 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:40,504 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.97.35\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:41,505 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:41,505 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.97.35\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:42,507 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:42,507 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.97.35\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:43,508 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:43,508 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.97.35\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:44,516 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:44,584 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:44,584 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:44,584 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:44,585 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:4', 'algo-2:4'] process_per_hosts: 4 num_processes: 8\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:44,586 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-10-11 11:43:44,638 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 4,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"validation-batch-size\": 512,\n",
      "        \"learning-rate\": 0.000125,\n",
      "        \"print-interval\": 50,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"train-batch-size\": 64,\n",
      "        \"epochs\": 40,\n",
      "        \"eval-batch-size\": 512\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"cifar10-tf-dist-2021-10-11-11-37-52-205\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-227612457811/cifar10-tf-dist-2021-10-11-11-37-52-205/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cifar10_tf2_sm_horovod\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cifar10_tf2_sm_horovod.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":40,\"eval-batch-size\":512,\"learning-rate\":0.000125,\"model_dir\":\"/opt/ml/model\",\"print-interval\":50,\"train-batch-size\":64,\"validation-batch-size\":512}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cifar10_tf2_sm_horovod.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"-verbose --NCCL_DEBUG=INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cifar10_tf2_sm_horovod\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-227612457811/cifar10-tf-dist-2021-10-11-11-37-52-205/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"-verbose --NCCL_DEBUG=INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":40,\"eval-batch-size\":512,\"learning-rate\":0.000125,\"model_dir\":\"/opt/ml/model\",\"print-interval\":50,\"train-batch-size\":64,\"validation-batch-size\":512},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-tf-dist-2021-10-11-11-37-52-205\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-227612457811/cifar10-tf-dist-2021-10-11-11-37-52-205/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_tf2_sm_horovod\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cifar10_tf2_sm_horovod.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"40\",\"--eval-batch-size\",\"512\",\"--learning-rate\",\"0.000125\",\"--model_dir\",\"/opt/ml/model\",\"--print-interval\",\"50\",\"--train-batch-size\",\"64\",\"--validation-batch-size\",\"512\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION-BATCH-SIZE=512\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.000125\u001b[0m\n",
      "\u001b[34mSM_HP_PRINT-INTERVAL=50\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN-BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=40\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL-BATCH-SIZE=512\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:4,algo-2:4 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -verbose -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_EVAL -x SM_CHANNEL_VALIDATION -x SM_CHANNEL_TRAIN -x SM_HP_VALIDATION-BATCH-SIZE -x SM_HP_LEARNING-RATE -x SM_HP_PRINT-INTERVAL -x SM_HP_MODEL_DIR -x SM_HP_TRAIN-BATCH-SIZE -x SM_HP_EPOCHS -x SM_HP_EVAL-BATCH-SIZE -x PYTHONPATH /usr/local/bin/python3.7 -m mpi4py cifar10_tf2_sm_horovod.py --epochs 40 --eval-batch-size 512 --learning-rate 0.000125 --model_dir /opt/ml/model --print-interval 50 --train-batch-size 64 --validation-batch-size 512\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-10-11 11:43:42 Training - Training image download completed. Training in progress.\u001b[35m2021-10-11 11:43:39.327702: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:39.332891: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:39.419946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:39.511484: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:43,050 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:43,641 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:43,641 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:43,648 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:43,719 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:43,719 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:43,719 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:43,719 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:43,727 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m Data for JOB [36089,1] offset 0 Total slots allocated 8\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-112-12#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [36089,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [36089,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [36089,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [36089,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\n",
      " Data for node: algo-2#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [36089,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [36089,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [36089,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [36089,1] App: 0 Process rank: 7 Bound: N/A\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:45,732 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=108, name='orted', status='sleeping', started='11:43:45')]\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:45,733 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=108, name='orted', status='sleeping', started='11:43:45')]\u001b[0m\n",
      "\u001b[35m2021-10-11 11:43:45,733 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=108, name='orted', status='sleeping', started='11:43:45')]\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## args: \u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:Namespace(epochs=40, eval='/opt/ml/input/data/eval', eval_batch_size=512, learning_rate=0.000125, model_dir='/opt/ml/model', model_output_dir='/opt/ml/model', momentum=0.9, optimizer='adam', print_interval=50, train='/opt/ml/input/data/train', train_batch_size=64, validation='/opt/ml/input/data/validation', validation_batch_size=512, weight_decay=0.0002)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## args: \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:Namespace(epochs=40, eval='/opt/ml/input/data/eval', eval_batch_size=512, learning_rate=0.000125, model_dir='/opt/ml/model', model_output_dir='/opt/ml/model', momentum=0.9, optimizer='adam', print_interval=50, train='/opt/ml/input/data/train', train_batch_size=64, validation='/opt/ml/input/data/validation', validation_batch_size=512, weight_decay=0.0002)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Channel Name: train\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Channel Name: train\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:buffer_size:  10000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:buffer_size:  10000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Channel Name: eval\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Channel Name: eval\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Channel Name: validation\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Channel Name: validation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Prepare Dataset ################[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches in train:  156[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches in eval:  19\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches in validation: [1,4]<stdout>: 19\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## num_train_batch on each GPU2 : 19 \u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## num_train_batch on each GPU4 : 19 \u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:################# Start Training  ################[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:################# Start Training  ################[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## num_train_batch on each GPU5 : 19 [1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## num_train_batch on each GPU1 : 19 [1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## num_train_batch on each GPU6 : 19 \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Prepare Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches in train:  156\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches in eval: [1,0]<stdout>: [1,0]<stdout>:19\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches in validation:  19\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Start Training  ################[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## num_train_batch on each GPU0 : 19 [1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## num_train_batch on each GPU3 : 19 \u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## num_train_batch on each GPU7 : 19 \u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-11 11:43:58.249 algo-2:112 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-11 11:43:58.249 algo-2:111 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-11 11:43:58.273 algo-2:113 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-11 11:43:58.329 algo-1:104 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-11 11:43:58.329 algo-1:103 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-11 11:43:58.337 algo-2:111 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-11 11:43:58.337 algo-2:112 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-11 11:43:58.337 algo-2:113 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-11 11:43:58.339 algo-2:112 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-11 11:43:58.339 algo-2:111 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-11 11:43:58.339 algo-2:113 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-11 11:43:58.340 algo-2:111 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-11 11:43:58.340 algo-2:112 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-11 11:43:58.340 algo-2:113 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-11 11:43:58.340 algo-2:112 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-11 11:43:58.341 algo-2:111 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-11 11:43:58.341 algo-2:113 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-11 11:43:58.341 algo-2:111 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-11 11:43:58.341 algo-2:112 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-11 11:43:58.341 algo-2:113 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-11 11:43:58.341 algo-2:112 INFO hook.py:413] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-11 11:43:58.341 algo-2:111 INFO hook.py:413] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-11 11:43:58.341 algo-2:113 INFO hook.py:413] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-11 11:43:58.406 algo-1:102 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-11 11:43:58.407 algo-2:114 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-11 11:43:58.415 algo-1:103 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-11 11:43:58.415 algo-1:104 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-11 11:43:58.417 algo-1:103 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-11 11:43:58.417 algo-1:104 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-11 11:43:58.417 algo-1:103 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-11 11:43:58.417 algo-1:104 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-11 11:43:58.418 algo-1:103 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-11 11:43:58.418 algo-1:104 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-11 11:43:58.418 algo-1:103 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-11 11:43:58.418 algo-1:104 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-11 11:43:58.419 algo-1:103 INFO hook.py:413] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-11 11:43:58.419 algo-1:104 INFO hook.py:413] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-11 11:43:58.427 algo-1:105 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-11 11:43:58.439 algo-1:102 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-11 11:43:58.440 algo-1:102 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-11 11:43:58.440 algo-2:114 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-11 11:43:58.441 algo-1:102 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-11 11:43:58.441 algo-2:114 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-11 11:43:58.441 algo-1:102 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-11 11:43:58.441 algo-1:102 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-11 11:43:58.441 algo-2:114 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-11 11:43:58.442 algo-1:102 INFO hook.py:413] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-11 11:43:58.442 algo-2:114 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-11 11:43:58.442 algo-2:114 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-11 11:43:58.443 algo-2:114 INFO hook.py:413] Monitoring the collections: metrics, losses, sm_metrics\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-11 11:43:58.458 algo-1:105 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-11 11:43:58.459 algo-1:105 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-11 11:43:58.459 algo-1:105 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-11 11:43:58.460 algo-1:105 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-11 11:43:58.460 algo-1:105 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-11 11:43:58.461 algo-1:105 INFO hook.py:413] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.112.12<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.112.12<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.112.12<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.112.12<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.112.12<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.97.35<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.97.35<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.97.35<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.97.35<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.112.12<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.112.12<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.112.12<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.97.35<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.97.35<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.97.35<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.97.35<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Trees [0] 2/4/-1->1->0|0->1->2/4/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1 [2] 2/4/-1->1->0|0->1->2/4/-1 [3] 2/-1/-1->1->0|0->1->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Trees [0] -1/-1/-1->7->6|6->7->-1/-1/-1 [1] -1/-1/-1->7->6|6->7->-1/-1/-1 [2] -1/-1/-1->7->6|6->7->-1/-1/-1 [3] -1/-1/-1->7->6|6->7->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->1|1->2->3/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Trees [0] -1/-1/-1->3->2|2->3->-1/-1/-1 [1] -1/-1/-1->3->2|2->3->-1/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->5|5->0->1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] 1/-1/-1->0->5|5->0->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Trees [0] 6/-1/-1->5->4|4->5->6/-1/-1 [1] 6/0/-1->5->4|4->5->6/0/-1 [2] 6/-1/-1->5->4|4->5->6/-1/-1 [3] 6/0/-1->5->4|4->5->6/0/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 7/-1/-1->6->5|5->6->7/-1/-1 [3] 7/-1/-1->6->5|5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Trees [0] 5/-1/-1->4->1|1->4->5/-1/-1 [1] 5/-1/-1->4->-1|-1->4->5/-1/-1 [2] 5/-1/-1->4->1|1->4->5/-1/-1 [3] 5/-1/-1->4->-1|-1->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 00 : 7[1e0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 00 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 00 : 7[1e0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 00 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Channel 00 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 00 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 00 : 4[1b0] -> 1[1c0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 00 : 4[1b0] -> 1[1c0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 01 : 7[1e0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Channel 01 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 01 : 7[1e0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 00 : 1[1c0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 00 : 1[1c0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 02 : 7[1e0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 02 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 01 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 01 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 01 : 0[1b0] -> 5[1c0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 02 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 01 : 0[1b0] -> 5[1c0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 01 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 02 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Channel 02 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 01 : 5[1c0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 02 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 01 : 5[1c0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 02 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Channel 02 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 02 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Channel 03 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 02 : 4[1b0] -> 1[1c0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 02 : 7[1e0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 02 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 03 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 02 : 4[1b0] -> 1[1c0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 02 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 03 : 7[1e0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 03 : 7[1e0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 03 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 02 : 1[1c0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 02 : 1[1c0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 03 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO comm 0x7f9b14348100 rank 7 nranks 8 cudaDev 3 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 03 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO Channel 03 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:384 [1] NCCL INFO comm 0x7f9ad833e4a0 rank 1 nranks 8 cudaDev 1 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 03 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO Channel 03 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 03 : 0[1b0] -> 5[1c0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:400 [2] NCCL INFO comm 0x7ff71c346490 rank 6 nranks 8 cudaDev 2 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 03 : 0[1b0] -> 5[1c0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 03 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Channel 03 : 5[1c0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO Channel 03 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:391 [0] NCCL INFO comm 0x7f8a5c346510 rank 4 nranks 8 cudaDev 0 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:383 [3] NCCL INFO comm 0x7f784c3402b0 rank 3 nranks 8 cudaDev 3 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO comm 0x7f7fd0348280 rank 2 nranks 8 cudaDev 2 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 03 : 5[1c0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO comm 0x7f10743672a0 rank 0 nranks 8 cudaDev 0 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO comm 0x7f7838345ff0 rank 5 nranks 8 cudaDev 1 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:382 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 88.511742\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 61.137840\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 72.099472\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 54.486580\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 73.761124\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 40.325096\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 41.902695\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 37.305397\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 1, Test Loss: 2.433474540710449, Test Accuracy: 11.04029655456543\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 1, Test Loss: 2.4286632537841797, Test Accuracy: 9.991776466369629\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.626735\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.479309\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.380824\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.326337\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.420871\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.437041\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.369349\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.358199\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 2, Test Loss: 2.297799825668335, Test Accuracy: 11.554276466369629\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 2, Test Loss: 2.297567844390869, Test Accuracy: 11.533717155456543\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.338253\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.317391\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.265206\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.292528\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.326491\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.317807\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.300799\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.320941\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 3, Test Loss: 2.2840592861175537, Test Accuracy: 12.325246810913086\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 3, Test Loss: 2.285593032836914, Test Accuracy: 12.335526466369629\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.332912\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.241621\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.343873\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.273756\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.261538\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.299872\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.302192\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.291824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 4, Test Loss: 2.2716991901397705, Test Accuracy: 12.633635520935059\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 4, Test Loss: 2.2670230865478516, Test Accuracy: 12.736431121826172\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.296853\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.260534\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.254070\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.300087\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.294688\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.245460\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.286839\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.249035\u001b[0m\n",
      "ProfilerReport-1633952272: Error\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 5, Test Loss: 2.2513744831085205, Test Accuracy: 14.340048789978027\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 5, Test Loss: 2.253594160079956, Test Accuracy: 13.620476722717285\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.223532\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.287255\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.279682\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.262069\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.220003\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.234147\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.325909\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.281739\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 6, Test Loss: 2.2135026454925537, Test Accuracy: 17.37253189086914\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 6, Test Loss: 2.2153494358062744, Test Accuracy: 17.125822067260742[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.188221\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.208990\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.197829\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.224548\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.229025\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.192774\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.171593\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.250729\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 7, Test Loss: 2.1444196701049805, Test Accuracy: 19.983552932739258\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 7, Test Loss: 2.1437413692474365, Test Accuracy: 19.91159439086914[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.966113\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.093258\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.152291\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.158538\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.159977\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.115374\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.234373\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.115077\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 8, Test Loss: 2.168511152267456, Test Accuracy: 21.710527420043945\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 8, Test Loss: 2.170832633972168, Test Accuracy: 21.175987243652344\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.166328\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.358232\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.008877\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.082623\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.235117\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.065044\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.268188\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.098912\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 9, Test Loss: 2.081894636154175, Test Accuracy: 23.478618621826172\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 9, Test Loss: 2.08876371383667, Test Accuracy: 22.985197067260742\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.244994\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.105899\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.087491\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.996949\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.189088\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.128327\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.109509\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.947455\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 10, Test Loss: 2.042139768600464, Test Accuracy: 25.21587371826172\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 10, Test Loss: 2.035848617553711, Test Accuracy: 24.835527420043945\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.042275\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.151665\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.084200\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.990281\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.224178\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.023804\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.128129\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.133193\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 11, Test Loss: 2.0198850631713867, Test Accuracy: 25.123353958129883\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 11, Test Loss: 2.036100387573242, Test Accuracy: 24.886924743652344\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.996386\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.045929\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.945342\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.006097\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.930257\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.077723\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.930748\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.943784\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 12, Test Loss: 2.0225884914398193, Test Accuracy: 25.986841201782227\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 12, Test Loss: 2.022306442260742, Test Accuracy: 25.935443878173828\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.969467\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.832194\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.032203\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.218576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0[1,6]<stdout>:#011[1,6]<stdout>:Loss: 2.010743[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.076272\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.020112\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.036392\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 13, Test Loss: 1.990822196006775, Test Accuracy: 26.726972579956055\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 13, Test Loss: 1.9865044355392456, Test Accuracy: 27.25123405456543\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.043808\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.025469\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.941727\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.132731\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.814557\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.083797\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.992441\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.069646\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 14, Test Loss: 1.970267415046692, Test Accuracy: 27.703535079956055[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 14, Test Loss: 1.9606136083602905, Test Accuracy: 27.63157844543457\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.072960\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.778863\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.952851\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.845439\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.852398\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.967930\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.917817\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.047505\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 15, Test Loss: 1.9143365621566772, Test Accuracy: 30.067846298217773\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 15, Test Loss: 1.9267046451568604, Test Accuracy: 29.90337371826172\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.911204\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.967117\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.932762\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.190343\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.002586\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.871507\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.937689\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.000040\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 16, Test Loss: 1.8653854131698608, Test Accuracy: 32.308799743652344\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 16, Test Loss: 1.8778020143508911, Test Accuracy: 31.7845401763916[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.926462\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.797885\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.809973\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.896726\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.679123\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.066780\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.698288\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.623661\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 17, Test Loss: 1.8429155349731445, Test Accuracy: 32.96669387817383\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 17, Test Loss: 1.8335322141647339, Test Accuracy: 33.706825256347656[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.606964\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.677397\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.817617\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.777473\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.128913\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.819163\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.721793\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.693158\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 18, Test Loss: 1.7788294553756714, Test Accuracy: 34.98149871826172[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 18, Test Loss: 1.7808711528778076, Test Accuracy: 34.56003189086914\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.801512\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.606019\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.990510\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.753085\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.732767\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.638926\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.890750\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.679123\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 19, Test Loss: 1.8580988645553589, Test Accuracy: 33.99465560913086[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 19, Test Loss: 1.849198579788208, Test Accuracy: 34.35443878173828\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.860808\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.986611\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.135013\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.805514\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.515883\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:#011Loss: 1.774317[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.718668\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.612989\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 20, Test Loss: 1.771498203277588, Test Accuracy: 33.94325637817383[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 20, Test Loss: 1.7734553813934326, Test Accuracy: 34.56003189086914\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.919853\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.749791\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.010798\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.773077\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.726239\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.820761\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.635327\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.769974\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 21, Test Loss: 1.7478094100952148, Test Accuracy: 35.96833801269531\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 21, Test Loss: 1.761566162109375, Test Accuracy: 34.89925765991211\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.680802\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.833941\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.833046\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.697278\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.735311\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.698951\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.731769\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.375671\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 22, Test Loss: 1.7256752252578735, Test Accuracy: 36.36924362182617\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 22, Test Loss: 1.7329039573669434, Test Accuracy: 36.060855865478516[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.740011\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.623781\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.727977\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.835054\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.548392\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.523518\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.649103\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.794333\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 23, Test Loss: 1.706357479095459, Test Accuracy: 38.05509948730469\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 23, Test Loss: 1.6988463401794434, Test Accuracy: 37.98313903808594\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.563791\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.909947\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.610742\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.736247\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.570335\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.647216\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.668716\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.533822\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 24, Test Loss: 1.7027294635772705, Test Accuracy: 37.46916198730469\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 24, Test Loss: 1.7015900611877441, Test Accuracy: 38.32236862182617\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.615977\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.981606\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.720744\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.518244\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.714414\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.658568\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.670591\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.568634\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 25, Test Loss: 1.6599464416503906, Test Accuracy: 39.895145416259766\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 25, Test Loss: 1.6649858951568604, Test Accuracy: 39.83346939086914\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.697478\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.567001\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.470547\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.674218\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.648870\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.657021\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.582988\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.458358\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 26, Test Loss: 1.613903284072876, Test Accuracy: 42.22861862182617\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 26, Test Loss: 1.611463189125061, Test Accuracy: 41.66324234008789\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.677901\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.289608\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.432244\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.456130\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.696161\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.799916\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.551334\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.372888\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 27, Test Loss: 1.6305961608886719, Test Accuracy: 41.519325256347656\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 27, Test Loss: 1.6251285076141357, Test Accuracy: 41.40625\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.678052\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.465619\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.631704\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.560866\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.517855\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.659328\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.736807\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.228343\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 28, Test Loss: 1.6582530736923218, Test Accuracy: 39.15501403808594\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 28, Test Loss: 1.6583305597305298, Test Accuracy: 39.24753189086914\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.631267\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.710276\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.561791\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.533590\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.799480\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.615554\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.587397\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.570511\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 29, Test Loss: 1.6145302057266235, Test Accuracy: 41.354854583740234\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 29, Test Loss: 1.6113905906677246, Test Accuracy: 40.86143112182617\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.591299\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.560752\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.682716\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.728758\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.403986\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.681495\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.632066\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.528123\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 30, Test Loss: 1.5771840810775757, Test Accuracy: 42.043582916259766\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 30, Test Loss: 1.585215449333191, Test Accuracy: 42.56784439086914\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.720802\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.305795\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.647899\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.474836\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.683496\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.380104\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.716389\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.469668\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 31, Test Loss: 1.571779489517212, Test Accuracy: 44.20230484008789\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 31, Test Loss: 1.5644068717956543, Test Accuracy: 44.18174362182617\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.836560\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.555101\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.538000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.468837\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.390896\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.282468\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.542999\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.649822\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 32, Test Loss: 1.6202573776245117, Test Accuracy: 41.354854583740234\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 32, Test Loss: 1.6139440536499023, Test Accuracy: 40.573604583740234\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.444631\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.638267\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.539519\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.523439\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.694261\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.750007\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.548634\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.377289\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 33, Test Loss: 1.574660062789917, Test Accuracy: 42.73231887817383\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 33, Test Loss: 1.5724594593048096, Test Accuracy: 43.236019134521484\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.617863\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.529783\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.325135\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.430917\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.536960\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.456614\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.570118\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.494025\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 34, Test Loss: 1.501886248588562, Test Accuracy: 46.792762756347656\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 34, Test Loss: 1.5076441764831543, Test Accuracy: 46.07318878173828[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.386485\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.326720\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.464244\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.369531\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.326197\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.237252\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.318416\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.290342\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 35, Test Loss: 1.5409009456634521, Test Accuracy: 44.89103698730469\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 35, Test Loss: 1.5359785556793213, Test Accuracy: 44.973270416259766\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.590167\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.487221\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.476001\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.334239\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.496993\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.714202\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.444037\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.392146\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 36, Test Loss: 1.5003153085708618, Test Accuracy: 47.50205612182617[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 36, Test Loss: 1.4961490631103516, Test Accuracy: 46.484375\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.186759\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.292618\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.425432\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.386868\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.429235\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.357384\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.440566\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.366493\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 37, Test Loss: 1.4759262800216675, Test Accuracy: 47.193668365478516[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 37, Test Loss: 1.4996064901351929, Test Accuracy: 46.71052551269531\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.224542\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.292278\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.352660\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.656317\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.119441\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.554101\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.252650\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.354549\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 38, Test Loss: 1.5120350122451782, Test Accuracy: 46.38157653808594\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 38, Test Loss: 1.5062971115112305, Test Accuracy: 46.18626403808594\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.382402\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.527288\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.295287\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.941404\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.508247\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.458913\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.307475\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.444852\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 39, Test Loss: 1.4681469202041626, Test Accuracy: 48.18050765991211\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 39, Test Loss: 1.4772400856018066, Test Accuracy: 48.44778060913086\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.540256\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.319086\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.383463\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.358639\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.562393\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.388632\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.356041\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.466225\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 40, Test Loss: 1.4718068838119507, Test Accuracy: 47.347862243652344\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Saving Model  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model is saved in /opt/ml/model[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 40, Test Loss: 1.4604862928390503, Test Accuracy: 47.25534439086914\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Saving Model  ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Model is saved in /opt/ml/model\u001b[0m\n",
      "\u001b[35m2021-10-11 11:45:22,031 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.97.35' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-10-11 11:43:45.844166: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-10-11 11:43:45.844175: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-10-11 11:43:45.844166: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-10-11 11:43:45.844334: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-10-11 11:43:45.844323: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-10-11 11:43:45.844332: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-11 11:43:45.844366: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-11 11:43:45.844487: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-11 11:43:45.845523: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-10-11 11:43:45.845529: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-10-11 11:43:45.845523: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-10-11 11:43:45.845527: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-10-11 11:43:45.845688: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-11 11:43:45.845688: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-10-11 11:43:45.845688: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-10-11 11:43:45.845688: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-10-11 11:43:45.884612: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-11 11:43:45.884913: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-10-11 11:43:45.884907: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-10-11 11:43:45.885153: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-10-11 11:43:45.886218: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-10-11 11:43:45.886218: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-11 11:43:45.886218: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-10-11 11:43:45.886434: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-11 11:45:20.183591: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-11 11:45:20.194573: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-11 11:45:22,023 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2021-10-11 11:45:52,047 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2021-10-11 11:45:52,047 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-10-11 11:46:02 Uploading - Uploading generated training model\n",
      "2021-10-11 11:46:02 Completed - Training job completed\n",
      "Training seconds: 638\n",
      "Billable seconds: 638\n"
     ]
    }
   ],
   "source": [
    "horovod_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 정리 작업\n",
    "\n",
    "## 모델 아티펙트 저장\n",
    "- S3 에 저장된 모델 아티펙트를 저장하여 추론시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horovod_artifact_path:  s3://sagemaker-us-east-1-227612457811/cifar10-tf-dist-2021-10-11-11-37-52-205/output/model.tar.gz\n",
      "Stored 'tf2_horovod_artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "tf2_horovod_artifact_path = horovod_estimator.model_data\n",
    "print(\"horovod_artifact_path: \", tf2_horovod_artifact_path)\n",
    "\n",
    "\n",
    "%store tf2_horovod_artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-11 11:45:57   12006151 cifar10-tf-dist-2021-10-11-11-37-52-205/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {tf2_horovod_artifact_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
