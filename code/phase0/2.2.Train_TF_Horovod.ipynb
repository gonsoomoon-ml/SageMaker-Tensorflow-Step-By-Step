{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 2.2] TF 호로보드로 분산 훈련 (로컬 모드 및 호스트 모드)\n",
    "\n",
    "\n",
    "본 워크샵의 모든 노트북은 **<font color=\"red\">conda_tensorflow2_p36</font>** 를 사용합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 1. 기본 환경 세팅 \n",
    "- 2. 데이터 세트를 S3 에 업로딩\n",
    "- 3. 노트북에서 세이지 메이커 스크립트 모드 스타일로 코드 변경\n",
    "- 4. 세이지 메이커 로컬 모드로 훈련\n",
    "- 5. 세이지 메이커의 호스트 모드로 훈련\n",
    "- 6. 모델 아티펙트 경로 저장\n",
    "\n",
    "\n",
    "## 참고:\n",
    "- 호로보드 깃의 TF2 공식 예시 입니다.\n",
    "    - [호로보드 공식 예제](https://github.com/horovod/horovod/blob/master/examples/tensorflow2/tensorflow2_mnist.py)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cnn-cifar10\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local_gpu\"\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_dir\n",
    "%store -r validation_dir\n",
    "%store -r eval_dir\n",
    "%store -r data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 세트를 S3에 업로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-057716757052/data/DEMO-cifar10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_location = sagemaker_session.upload_data(path=data_dir, key_prefix='data/DEMO-cifar10')\n",
    "display(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 훈련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local_gpu\"\n",
    "# instance_type = \"ml.p3.8xlarge\"\n",
    "\n",
    "job_name ='cifar10-horovod'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시스템의 이전 도커 컨테이너 삭제\n",
    "- 아래와 같은 명령어를 사용하여 저장 공간을 확보 합니다.\n",
    "- 필요시 주석을 제거하고 사용하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도커 컨테이너 모두 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs        241G   80K  241G   1% /dev\n",
      "tmpfs           241G     0  241G   0% /dev/shm\n",
      "/dev/xvda1      109G  108G  863M 100% /\n",
      "/dev/xvdf       984G   25G  910G   3% /home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "! df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! docker container prune -f\n",
    "# ! df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도커 이미지 모두 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! df -h\n",
    "# ! docker image prune -f --all\n",
    "# ! df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 로컬모드로 훈련 \n",
    "- 현 실행 노트북 인스턴스에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_gpu_learning_rate:  0.000125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count ):\n",
    "    total_gpu = num_gpu * train_instance_count\n",
    "\n",
    "    multi_gpu_learning_rate = one_gpu_learning_rate / total_gpu\n",
    "    print(\"multi_gpu_learning_rate: \", multi_gpu_learning_rate)\n",
    "    \n",
    "    return multi_gpu_learning_rate\n",
    "\n",
    "train_instance_type = 'ml.p3.16xlarge'\n",
    "num_gpu = 8\n",
    "train_instance_count = 1\n",
    "one_gpu_learning_rate = 0.001 \n",
    "\n",
    "multi_gpu_learning_rate = calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributions has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating hwzq54ccaf-algo-1-0an7p ... \n",
      "Creating hwzq54ccaf-algo-1-0an7p ... done\n",
      "Attaching to hwzq54ccaf-algo-1-0an7p\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:39.058824: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:39.059018: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:39.063529: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:39.100874: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:40,833 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:41,267 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:41,267 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:41,275 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:41,276 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1-0an7p'] Hosts: ['algo-1-0an7p:8'] process_per_hosts: 8 num_processes: 8\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:41,278 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:02:41,365 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m Training Env:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m {\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"additional_framework_parameters\": {\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"sagemaker_mpi_enabled\": true,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"sagemaker_mpi_num_of_processes_per_host\": 8,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"sagemaker_mpi_custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\"\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     },\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"validation\": \"/opt/ml/input/data/validation\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"eval\": \"/opt/ml/input/data/eval\"\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     },\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"current_host\": \"algo-1-0an7p\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"algo-1-0an7p\"\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     ],\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"epochs\": 10,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"learning-rate\": 0.000125,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"print-interval\": 100,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"train-batch-size\": 64,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"eval-batch-size\": 512,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"validation-batch-size\": 512,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"model_dir\": \"/opt/ml/model\"\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     },\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"train\": {\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         },\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"validation\": {\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         },\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"eval\": {\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         }\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     },\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"job_name\": \"cifar10-tf-dist-2021-10-10-05-02-32-608\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"master_hostname\": \"algo-1-0an7p\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/cifar10-tf-dist-2021-10-10-05-02-32-608/source/sourcedir.tar.gz\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"module_name\": \"cifar10_tf2_sm_horovod\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"current_host\": \"algo-1-0an7p\",\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m             \"algo-1-0an7p\"\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m         ]\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     },\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m     \"user_entry_point\": \"cifar10_tf2_sm_horovod.py\"\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m }\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m Environment variables:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_HOSTS=[\"algo-1-0an7p\"]\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_HPS={\"epochs\":10,\"eval-batch-size\":512,\"learning-rate\":0.000125,\"model_dir\":\"/opt/ml/model\",\"print-interval\":100,\"train-batch-size\":64,\"validation-batch-size\":512}\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_USER_ENTRY_POINT=cifar10_tf2_sm_horovod.py\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"-verbose --NCCL_DEBUG=INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-0an7p\",\"hosts\":[\"algo-1-0an7p\"]}\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_INPUT_DATA_CONFIG={\"eval\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_CHANNELS=[\"eval\",\"train\",\"validation\"]\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_CURRENT_HOST=algo-1-0an7p\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_MODULE_NAME=cifar10_tf2_sm_horovod\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/cifar10-tf-dist-2021-10-10-05-02-32-608/source/sourcedir.tar.gz\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"-verbose --NCCL_DEBUG=INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1-0an7p\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-0an7p\"],\"hyperparameters\":{\"epochs\":10,\"eval-batch-size\":512,\"learning-rate\":0.000125,\"model_dir\":\"/opt/ml/model\",\"print-interval\":100,\"train-batch-size\":64,\"validation-batch-size\":512},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-tf-dist-2021-10-10-05-02-32-608\",\"log_level\":20,\"master_hostname\":\"algo-1-0an7p\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-tf-dist-2021-10-10-05-02-32-608/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_tf2_sm_horovod\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-0an7p\",\"hosts\":[\"algo-1-0an7p\"]},\"user_entry_point\":\"cifar10_tf2_sm_horovod.py\"}\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"10\",\"--eval-batch-size\",\"512\",\"--learning-rate\",\"0.000125\",\"--model_dir\",\"/opt/ml/model\",\"--print-interval\",\"100\",\"--train-batch-size\",\"64\",\"--validation-batch-size\",\"512\"]\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_CHANNEL_EVAL=/opt/ml/input/data/eval\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_HP_EPOCHS=10\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_HP_LEARNING-RATE=0.000125\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_HP_PRINT-INTERVAL=100\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_HP_TRAIN-BATCH-SIZE=64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_HP_EVAL-BATCH-SIZE=512\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_HP_VALIDATION-BATCH-SIZE=512\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m SM_HP_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m mpirun --host algo-1-0an7p:8 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -verbose -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN -x SM_CHANNEL_VALIDATION -x SM_CHANNEL_EVAL -x SM_HP_EPOCHS -x SM_HP_LEARNING-RATE -x SM_HP_PRINT-INTERVAL -x SM_HP_TRAIN-BATCH-SIZE -x SM_HP_EVAL-BATCH-SIZE -x SM_HP_VALIDATION-BATCH-SIZE -x SM_HP_MODEL_DIR -x PYTHONPATH /usr/local/bin/python3.7 -m mpi4py cifar10_tf2_sm_horovod.py --epochs 10 --eval-batch-size 512 --learning-rate 0.000125 --model_dir /opt/ml/model --print-interval 100 --train-batch-size 64 --validation-batch-size 512\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  Data for JOB [23288,1] offset 0 Total slots allocated 8\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  ========================   JOB MAP   ========================\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  Data for node: f32c7eddbee5\tNum slots: 8\tMax slots: 0\tNum procs: 8\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  \tProcess OMPI jobid: [23288,1] App: 0 Process rank: 0 Bound: N/A\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  \tProcess OMPI jobid: [23288,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  \tProcess OMPI jobid: [23288,1] App: 0 Process rank: 2 Bound: N/A\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  \tProcess OMPI jobid: [23288,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  \tProcess OMPI jobid: [23288,1] App: 0 Process rank: 4 Bound: N/A\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  \tProcess OMPI jobid: [23288,1] App: 0 Process rank: 5 Bound: N/A\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  \tProcess OMPI jobid: [23288,1] App: 0 Process rank: 6 Bound: N/A\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  \tProcess OMPI jobid: [23288,1] App: 0 Process rank: 7 Bound: N/A\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m  =============================================================\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## args: \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>: Namespace(epochs=10, eval='/opt/ml/input/data/eval', eval_batch_size=512, learning_rate=0.000125, model_dir='/opt/ml/model', model_output_dir='/opt/ml/model', momentum=0.9, optimizer='adam', print_interval=100, train='/opt/ml/input/data/train', train_batch_size=64, validation='/opt/ml/input/data/validation', validation_batch_size=512, weight_decay=0.0002)\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:################# Loading Dataset ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:Channel Name: train\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:# of batches loading TFRecord : 40000\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:buffer_size:  40000\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:################# Loading Dataset ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:Channel Name: eval\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:# of batches loading TFRecord : 10000\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:################# Loading Dataset ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:Channel Name: validation\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:# of batches loading TFRecord : 10000\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## num_train_batch on each GPU1 : 78 [1,1]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## num_train_batch on each GPU3 : 78 \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:################# Start Training  ################[1,2]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:################# Prepare Dataset ################[1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:# of batches in train: [1,0]<stdout>: 625\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:# of batches in eval: [1,0]<stdout>: 19\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:# of batches in validation:  19\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## num_train_batch on each GPU2 : 78 [1,2]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## num_train_batch on each GPU0 : 78 \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:################# Start Training  ################[1,4]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## num_train_batch on each GPU4 : 78 [1,4]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## num_train_batch on each GPU6 : 78 \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:################# Start Training  ################[1,5]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## num_train_batch on each GPU5 : 78 \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## num_train_batch on each GPU7 : 78 [1,7]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:[2021-10-10 05:03:03.568 algo-1-0an7p:156 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:[2021-10-10 05:03:03.603 algo-1-0an7p:156 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:[2021-10-10 05:03:03.615 algo-1-0an7p:158 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:[2021-10-10 05:03:03.648 algo-1-0an7p:158 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:[2021-10-10 05:03:03.796 algo-1-0an7p:161 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:[2021-10-10 05:03:03.828 algo-1-0an7p:161 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:[2021-10-10 05:03:03.831 algo-1-0an7p:160 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:[2021-10-10 05:03:03.846 algo-1-0an7p:155 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:[2021-10-10 05:03:03.862 algo-1-0an7p:157 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:[2021-10-10 05:03:03.863 algo-1-0an7p:160 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:[2021-10-10 05:03:03.878 algo-1-0an7p:155 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:[2021-10-10 05:03:03.896 algo-1-0an7p:157 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:[2021-10-10 05:03:03.974 algo-1-0an7p:162 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:[2021-10-10 05:03:04.007 algo-1-0an7p:159 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:[2021-10-10 05:03:04.007 algo-1-0an7p:162 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:[2021-10-10 05:03:04.040 algo-1-0an7p:159 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Using network Socket\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:NCCL version 2.7.8+cuda11.0\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Using network Socket\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Using network Socket\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Using network Socket\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Using network Socket\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Using network Socket\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Using network Socket\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Using network Socket\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:algo-1-0an7p:156:1252 [1] NCCL INFO comm 0x7fefc0347000 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO comm 0x7f26dc358700 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:algo-1-0an7p:157:1246 [2] NCCL INFO comm 0x7fd3c4349250 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:algo-1-0an7p:158:1253 [3] NCCL INFO comm 0x7f0ba0347000 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:algo-1-0an7p:159:1258 [4] NCCL INFO comm 0x7f6a9c3475b0 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:algo-1-0an7p:161:1243 [6] NCCL INFO comm 0x7f41b4346f90 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:algo-1-0an7p:162:1264 [7] NCCL INFO comm 0x7f0cc0347000 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:algo-1-0an7p:160:1244 [5] NCCL INFO comm 0x7fca983492a0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:algo-1-0an7p:155:1261 [0] NCCL INFO Launch mode Parallel\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 63.307297\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 75.839996\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 52.848732\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 34.423820\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 56.746830\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 40.066673\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 82.707230\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 40.368336\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 1, Test Loss: 2.2779364585876465, Test Accuracy: 13.579358100891113\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 2.279063\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 2.309744\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 2.270771\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 2.292920\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 2.284425\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 2.269158\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 2.266419\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 2.264992\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 2, Test Loss: 1.998761773109436, Test Accuracy: 26.840049743652344\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 2.187376\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.965646\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 2.230516\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 2.010676\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 2.096054\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.818006\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 2.035138\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 2.013900\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 3, Test Loss: 1.7812367677688599, Test Accuracy: 34.98149871826172\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.600994\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.887186\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 2.032309\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.812523\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.743907\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.857797\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.798096\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.723854\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 4, Test Loss: 1.6500110626220703, Test Accuracy: 40.60443878173828[1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.666252\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.650829\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.703204\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.706475\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.581214\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.639020\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.843732\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.345359\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 5, Test Loss: 1.5617930889129639, Test Accuracy: 43.236019134521484\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.564862\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.841840\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.663003\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.528399\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.508401\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.411660\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 2.042641\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.723439\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 6, Test Loss: 1.533787727355957, Test Accuracy: 45.21998596191406\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.171962\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.662168\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.615036\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.519931\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.544619\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.335829\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.651577\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.628185\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 7, Test Loss: 1.4805054664611816, Test Accuracy: 47.09087371826172\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.263923\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.565020\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.458502\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.388094\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.441043\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.197102\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.407402\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.289871[1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 8, Test Loss: 1.4583654403686523, Test Accuracy: 48.22162628173828[1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.752952\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.547172\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.381939\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.337324\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.524963\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.561429\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.529189\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.287720\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 9, Test Loss: 1.4028736352920532, Test Accuracy: 50.38034439086914\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.026367\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.416010\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.595199\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.539502\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.343668\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.138776\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.280000\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.572682\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:Training Finished.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:Training Finished.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:Training Finished.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:Training Finished.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:Training Finished.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:Training Finished.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:Training Finished.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:## Epoch 10, Test Loss: 1.3440144062042236, Test Accuracy: 51.42886734008789\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:Training Finished.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:################# Start Training  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:################# Saving Model  ################\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stdout>:Model is saved in /opt/ml/model\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stderr>:2021-10-10 05:02:41.613006: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stderr>:2021-10-10 05:02:41.613005: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stderr>:2021-10-10 05:02:41.613006: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stderr>:2021-10-10 05:02:41.613008: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stderr>:2021-10-10 05:02:41.613187: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stderr>:2021-10-10 05:02:41.613188: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stderr>:2021-10-10 05:02:41.613188: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stderr>:2021-10-10 05:02:41.613188: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stderr>:2021-10-10 05:02:41.616131: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stderr>:2021-10-10 05:02:41.616132: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stderr>:2021-10-10 05:02:41.616131: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stderr>:2021-10-10 05:02:41.616131: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stderr>:2021-10-10 05:02:41.616309: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stderr>:2021-10-10 05:02:41.616310: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stderr>:2021-10-10 05:02:41.616309: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stderr>:2021-10-10 05:02:41.616310: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,4]<stderr>:2021-10-10 05:02:41.654577: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,1]<stderr>:2021-10-10 05:02:41.654576: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,2]<stderr>:2021-10-10 05:02:41.654910: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,3]<stderr>:2021-10-10 05:02:41.655305: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,6]<stderr>:2021-10-10 05:02:41.657162: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stderr>:2021-10-10 05:02:41.657161: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,7]<stderr>:2021-10-10 05:02:41.657479: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,5]<stderr>:2021-10-10 05:02:41.657778: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stderr>:2021-10-10 05:03:52.448664: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m [1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m \n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p |\u001b[0m 2021-10-10 05:03:54,460 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mhwzq54ccaf-algo-1-0an7p exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "hyperparameters = {\n",
    "                    'epochs' : 10,\n",
    "                    'learning-rate' : float(f\"{multi_gpu_learning_rate}\"),\n",
    "                    'print-interval' : 100,\n",
    "                    'train-batch-size': 64,    \n",
    "                    'eval-batch-size': 512,        \n",
    "                    'validation-batch-size': 512,\n",
    "                  }\n",
    "\n",
    "distributions = {\n",
    "    'mpi': {\n",
    "        'enabled': True, \n",
    "        'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO',\n",
    "        'processes_per_host': int(f\"{num_gpu}\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Change base_job_name to 'cifar10-dist' for console visibility\n",
    "estimator = TensorFlow(base_job_name='cifar10-tf-dist',\n",
    "                       entry_point='cifar10_tf2_sm_horovod.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       framework_version='2.4.1',\n",
    "                       py_version='py37',\n",
    "                       script_mode=True,                            \n",
    "                       hyperparameters= hyperparameters,\n",
    "                       train_instance_count=1,   # 변경\n",
    "                       train_instance_type='local_gpu',\n",
    "                       distributions=distributions # 추가\n",
    "                      )\n",
    "\n",
    "\n",
    "estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬모드에서 도커 이미지 다운로드 된 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                         TAG                 IMAGE ID            CREATED             SIZE\n",
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training   2.4.1-gpu-py37      8467bc1c5070        5 months ago        8.91GB\n"
     ]
    }
   ],
   "source": [
    "! docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 호스트 모드로 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {'Name': 'train:loss', 'Regex': 'loss: (.*?) '},\n",
    "    {'Name': 'train:accuracy', 'Regex': 'acc: (.*?) '},\n",
    "    {'Name': 'validation:loss', 'Regex': 'val_loss: (.*?) '},\n",
    "    {'Name': 'validation:accuracy', 'Regex': 'val_acc: (.*?) '}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_gpu_learning_rate:  0.000125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count ):\n",
    "    total_gpu = num_gpu * train_instance_count\n",
    "\n",
    "    multi_gpu_learning_rate = one_gpu_learning_rate / total_gpu\n",
    "    print(\"multi_gpu_learning_rate: \", multi_gpu_learning_rate)\n",
    "    \n",
    "    return multi_gpu_learning_rate\n",
    "\n",
    "train_instance_type = 'ml.p3.8xlarge'\n",
    "num_gpu = 4\n",
    "train_instance_count = 2\n",
    "total_num_gpu = num_gpu * train_instance_count\n",
    "one_gpu_learning_rate = 0.001 \n",
    "\n",
    "multi_gpu_learning_rate = calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "                    'epochs' : 40,\n",
    "                    'learning-rate' : float(f\"{multi_gpu_learning_rate}\"),\n",
    "                    'print-interval' : 50,\n",
    "                    'train-batch-size': 64,    \n",
    "                    'eval-batch-size': 512,        \n",
    "                    'validation-batch-size': 512,\n",
    "                  }\n",
    "\n",
    "\n",
    "distributions = {\n",
    "    'mpi': {\n",
    "        'enabled': True, \n",
    "        'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO',\n",
    "        'processes_per_host': int(f\"{num_gpu}\")\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributions has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "\n",
    "horovod_estimator = TensorFlow(base_job_name='cifar10-tf-dist',\n",
    "                       entry_point='cifar10_tf2_sm_horovod.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       framework_version='2.4.1',\n",
    "                       py_version='py37',\n",
    "                       script_mode=True,                            \n",
    "                       hyperparameters= hyperparameters,\n",
    "                       train_instance_count=train_instance_count,   # 변경\n",
    "                       train_instance_type= train_instance_type,\n",
    "                       distributions=distributions # 추가\n",
    "                      )\n",
    "\n",
    "\n",
    "horovod_estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-10 05:03:56 Starting - Starting the training job...\n",
      "2021-10-10 05:04:20 Starting - Launching requested ML instancesProfilerReport-1633842236: InProgress\n",
      ".........\n",
      "2021-10-10 05:05:41 Starting - Preparing the instances for training.........\n",
      "2021-10-10 05:07:21 Downloading - Downloading input data\n",
      "2021-10-10 05:07:21 Training - Downloading the training image...............\u001b[34m2021-10-10 05:09:46.208416: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:09:46.213148: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:09:46.307321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-10-10 05:09:46.405215: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:09:50,028 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:06.401163: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:06.422952: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:06.941282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:07.262427: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\n",
      "2021-10-10 05:10:22 Training - Training image download completed. Training in progress.\u001b[35m2021-10-10 05:10:15,966 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:15,967 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:15,968 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:15,968 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.66.146\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:16,970 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:16,970 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.66.146\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:17,971 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:17,971 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.66.146\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:17,838 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:18,981 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:19,076 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:19,077 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:19,077 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:19,077 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:19,085 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:18,843 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:18,843 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:18,852 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:18,853 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:18,854 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.2.127.148\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:19,862 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:19,941 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:19,941 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:19,941 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:19,942 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:4', 'algo-2:4'] process_per_hosts: 4 num_processes: 8\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:19,943 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-10-10 05:10:19,994 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 4,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"validation-batch-size\": 512,\n",
      "        \"learning-rate\": 0.000125,\n",
      "        \"print-interval\": 50,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"train-batch-size\": 64,\n",
      "        \"epochs\": 40,\n",
      "        \"eval-batch-size\": 512\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"cifar10-tf-dist-2021-10-10-05-03-55-991\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/cifar10-tf-dist-2021-10-10-05-03-55-991/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cifar10_tf2_sm_horovod\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cifar10_tf2_sm_horovod.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":40,\"eval-batch-size\":512,\"learning-rate\":0.000125,\"model_dir\":\"/opt/ml/model\",\"print-interval\":50,\"train-batch-size\":64,\"validation-batch-size\":512}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cifar10_tf2_sm_horovod.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"-verbose --NCCL_DEBUG=INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cifar10_tf2_sm_horovod\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/cifar10-tf-dist-2021-10-10-05-03-55-991/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"-verbose --NCCL_DEBUG=INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":40,\"eval-batch-size\":512,\"learning-rate\":0.000125,\"model_dir\":\"/opt/ml/model\",\"print-interval\":50,\"train-batch-size\":64,\"validation-batch-size\":512},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-tf-dist-2021-10-10-05-03-55-991\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-tf-dist-2021-10-10-05-03-55-991/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_tf2_sm_horovod\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cifar10_tf2_sm_horovod.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"40\",\"--eval-batch-size\",\"512\",\"--learning-rate\",\"0.000125\",\"--model_dir\",\"/opt/ml/model\",\"--print-interval\",\"50\",\"--train-batch-size\",\"64\",\"--validation-batch-size\",\"512\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION-BATCH-SIZE=512\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.000125\u001b[0m\n",
      "\u001b[34mSM_HP_PRINT-INTERVAL=50\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN-BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=40\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL-BATCH-SIZE=512\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:4,algo-2:4 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -verbose -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_EVAL -x SM_CHANNEL_VALIDATION -x SM_CHANNEL_TRAIN -x SM_HP_VALIDATION-BATCH-SIZE -x SM_HP_LEARNING-RATE -x SM_HP_PRINT-INTERVAL -x SM_HP_MODEL_DIR -x SM_HP_TRAIN-BATCH-SIZE -x SM_HP_EPOCHS -x SM_HP_EVAL-BATCH-SIZE -x PYTHONPATH /usr/local/bin/python3.7 -m mpi4py cifar10_tf2_sm_horovod.py --epochs 40 --eval-batch-size 512 --learning-rate 0.000125 --model_dir /opt/ml/model --print-interval 50 --train-batch-size 64 --validation-batch-size 512\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:21,091 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=108, name='orted', status='disk-sleep', started='05:10:20')]\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:21,091 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=108, name='orted', status='running', started='05:10:20')]\u001b[0m\n",
      "\u001b[35m2021-10-10 05:10:21,091 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=108, name='orted', status='running', started='05:10:20')]\u001b[0m\n",
      "\u001b[34m Data for JOB [47098,1] offset 0 Total slots allocated 8\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-2-66-146#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [47098,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [47098,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [47098,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [47098,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\n",
      " Data for node: algo-2#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [47098,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [47098,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [47098,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [47098,1] App: 0 Process rank: 7 Bound: N/A\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## args: \u001b[0m\n",
      "\u001b[34m[1,4]<stdout>: [1,4]<stdout>:Namespace(epochs=40, eval='/opt/ml/input/data/eval', eval_batch_size=512, learning_rate=0.000125, model_dir='/opt/ml/model', model_output_dir='/opt/ml/model', momentum=0.9, optimizer='adam', print_interval=50, train='/opt/ml/input/data/train', train_batch_size=64, validation='/opt/ml/input/data/validation', validation_batch_size=512, weight_decay=0.0002)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## args: \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1,0]<stdout>:Namespace(epochs=40, eval='/opt/ml/input/data/eval', eval_batch_size=512, learning_rate=0.000125, model_dir='/opt/ml/model', model_output_dir='/opt/ml/model', momentum=0.9, optimizer='adam', print_interval=50, train='/opt/ml/input/data/train', train_batch_size=64, validation='/opt/ml/input/data/validation', validation_batch_size=512, weight_decay=0.0002)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Channel Name: train\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches loading TFRecord : 40000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Channel Name: train\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches loading TFRecord : 40000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:buffer_size:  40000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:buffer_size:  40000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Channel Name: eval\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Channel Name: eval\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Channel Name: validation\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## num_train_batch on each GPU6 : 78 [1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## num_train_batch on each GPU5 : 78 \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Channel Name: validation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:################# Start Training  ################[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## num_train_batch on each GPU7 : 78 \u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## num_train_batch on each GPU3 : 78 \u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Prepare Dataset ################[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches in train:  625\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches in eval:  19\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:# of batches in validation: [1,4]<stdout>: 19\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## num_train_batch on each GPU4 : 78 \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Prepare Dataset ################[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches in train: [1,0]<stdout>: [1,0]<stdout>:625[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches in eval: [1,0]<stdout>: 19[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches in validation:  19\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Start Training  ################[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## num_train_batch on each GPU0 : 78 \u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:################# Start Training  ################[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## num_train_batch on each GPU1 : 78 \u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## num_train_batch on each GPU2 : 78 \u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-10 05:10:43.367 algo-2:112 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-10 05:10:43.367 algo-2:113 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-10 05:10:43.437 algo-2:114 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-10 05:10:43.465 algo-2:113 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-10 05:10:43.465 algo-2:112 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-10 05:10:43.466 algo-2:113 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-10 05:10:43.466 algo-2:112 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-10 05:10:43.467 algo-2:113 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-10 05:10:43.467 algo-2:112 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-10 05:10:43.467 algo-2:111 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-10 05:10:43.467 algo-2:112 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-10 05:10:43.467 algo-2:113 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-10 05:10:43.468 algo-2:112 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-10 05:10:43.468 algo-2:113 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-10-10 05:10:43.468 algo-2:112 INFO hook.py:413] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-10-10 05:10:43.468 algo-2:113 INFO hook.py:413] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-10 05:10:43.471 algo-2:114 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-10 05:10:43.472 algo-2:114 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-10 05:10:43.472 algo-2:114 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-10 05:10:43.473 algo-2:114 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-10 05:10:43.473 algo-2:114 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-10-10 05:10:43.473 algo-2:114 INFO hook.py:413] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-10 05:10:43.500 algo-2:111 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-10 05:10:43.501 algo-2:111 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-10 05:10:43.501 algo-2:111 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-10 05:10:43.502 algo-2:111 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-10 05:10:43.502 algo-2:111 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-10-10 05:10:43.502 algo-2:111 INFO hook.py:413] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-10 05:10:44.260 algo-1:103 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-10 05:10:44.260 algo-1:105 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-10 05:10:44.260 algo-1:102 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-10 05:10:44.260 algo-1:104 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-10 05:10:44.412 algo-1:102 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-10 05:10:44.412 algo-1:103 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-10 05:10:44.412 algo-1:105 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-10 05:10:44.412 algo-1:104 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-10 05:10:44.419 algo-1:105 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-10 05:10:44.419 algo-1:102 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-10 05:10:44.419 algo-1:103 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-10 05:10:44.419 algo-1:104 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-10 05:10:44.419 algo-1:103 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-10 05:10:44.419 algo-1:105 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-10 05:10:44.419 algo-1:102 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-10 05:10:44.419 algo-1:104 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-10 05:10:44.420 algo-1:102 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-10 05:10:44.420 algo-1:103 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-10 05:10:44.420 algo-1:105 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-10 05:10:44.420 algo-1:104 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-10 05:10:44.420 algo-1:103 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-10 05:10:44.420 algo-1:105 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-10 05:10:44.420 algo-1:102 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-10 05:10:44.420 algo-1:104 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-10-10 05:10:44.420 algo-1:103 INFO hook.py:413] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-10-10 05:10:44.420 algo-1:105 INFO hook.py:413] Monitoring the collections: losses, metrics, sm_metrics\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-10-10 05:10:44.420 algo-1:102 INFO hook.py:413] Monitoring the collections: losses, sm_metrics, metrics\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-10-10 05:10:44.420 algo-1:104 INFO hook.py:413] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.66.146<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.66.146<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.127.148<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.127.148<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.127.148<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Bootstrap : Using [0]eth0:10.2.127.148<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.127.148<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.127.148<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO NET/Socket : Using [0]eth0:10.2.127.148<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.127.148<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Bootstrap : Using [0]eth0:10.2.66.146<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Bootstrap : Using [0]eth0:10.2.66.146<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Bootstrap : Using [0]eth0:10.2.66.146<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] ofi_init:1136 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO NET/Socket : Using [0]eth0:10.2.66.146<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO NET/Socket : Using [0]eth0:10.2.66.146<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO NET/Socket : Using [0]eth0:10.2.66.146<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Trees [0] 2/4/-1->1->0|0->1->2/4/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1 [2] 2/4/-1->1->0|0->1->2/4/-1 [3] 2/-1/-1->1->0|0->1->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Trees [0] -1/-1/-1->3->2|2->3->-1/-1/-1 [1] -1/-1/-1->3->2|2->3->-1/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->1|1->2->3/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 7/-1/-1->6->5|5->6->7/-1/-1 [3] 7/-1/-1->6->5|5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->5|5->0->1/-1/-1 [2] 1/-1/-1->0->-1|-1->0->1/-1/-1 [3] 1/-1/-1->0->5|5->0->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Trees [0] 5/-1/-1->4->1|1->4->5/-1/-1 [1] 5/-1/-1->4->-1|-1->4->5/-1/-1 [2] 5/-1/-1->4->1|1->4->5/-1/-1 [3] 5/-1/-1->4->-1|-1->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Trees [0] 6/-1/-1->5->4|4->5->6/-1/-1 [1] 6/0/-1->5->4|4->5->6/0/-1 [2] 6/-1/-1->5->4|4->5->6/-1/-1 [3] 6/0/-1->5->4|4->5->6/0/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Trees [0] -1/-1/-1->7->6|6->7->-1/-1/-1 [1] -1/-1/-1->7->6|6->7->-1/-1/-1 [2] -1/-1/-1->7->6|6->7->-1/-1/-1 [3] -1/-1/-1->7->6|6->7->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 00 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 00 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 00 : 7[1e0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 00 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 00 : 7[1e0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Channel 00 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 00 : 4[1b0] -> 1[1c0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 01 : 7[1e0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 00 : 4[1b0] -> 1[1c0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 00 : 1[1c0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 01 : 7[1e0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 02 : 7[1e0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 00 : 1[1c0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Channel 01 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 01 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 01 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 02 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 02 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 02 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 01 : 0[1b0] -> 5[1c0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 01 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 01 : 0[1b0] -> 5[1c0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Channel 02 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 01 : 5[1c0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 02 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 01 : 5[1c0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 02 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Channel 02 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 02 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Channel 03 : 3[1e0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 02 : 4[1b0] -> 1[1c0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 02 : 7[1e0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 02 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 03 : 2[1d0] -> 3[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 02 : 4[1b0] -> 1[1c0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 02 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 03 : 7[1e0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 03 : 7[1e0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 02 : 1[1c0] -> 4[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 02 : 1[1c0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 03 : 0[1b0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:114:392 [3] NCCL INFO comm 0x7f66943471b0 rank 7 nranks 8 cudaDev 3 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 03 : 1[1c0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO Channel 03 : 2[1d0] -> 1[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO Channel 03 : 1[1c0] -> 0[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:103:382 [1] NCCL INFO comm 0x7fcae833d440 rank 1 nranks 8 cudaDev 1 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 03 : 3[1e0] -> 4[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO Channel 03 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:113:391 [2] NCCL INFO comm 0x7f41403454b0 rank 6 nranks 8 cudaDev 2 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 03 : 0[1b0] -> 5[1c0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 03 : 0[1b0] -> 5[1c0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 03 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Channel 03 : 5[1c0] -> 0[1b0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO Channel 03 : 3[1e0] -> 2[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:111:394 [0] NCCL INFO comm 0x7ff7e0345680 rank 4 nranks 8 cudaDev 0 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:104:385 [2] NCCL INFO comm 0x7f79a0345650 rank 2 nranks 8 cudaDev 2 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:105:384 [3] NCCL INFO comm 0x7fd6b833d4f0 rank 3 nranks 8 cudaDev 3 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO Channel 03 : 5[1c0] -> 0[1b0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO comm 0x7f07143677e0 rank 0 nranks 8 cudaDev 0 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:112:393 [1] NCCL INFO comm 0x7fa5e43471e0 rank 5 nranks 8 cudaDev 1 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:102:383 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 42.576740\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 50.438835\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 47.165115\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 32.364681\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 39.895802\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 42.311127\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 55.598988\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 55.587582\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 2.295617\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 2.294741\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 2.307439\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 2.315461\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 2.278943\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 2.284650\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 2.277230\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 2.314284\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 1, Test Loss: 2.2500505447387695, Test Accuracy: 14.997943878173828[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 1, Test Loss: 2.2531309127807617, Test Accuracy: 14.504523277282715\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.328621\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.235237\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.166879\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.234261\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.349641\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.261973\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.255656\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.300442\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 2.291332\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 2.204888\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 2.206476\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 2.306323\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 2.035293\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 2.161648\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 2.164474\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 2.186240\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 2, Test Loss: 2.1253459453582764, Test Accuracy: 22.831003189086914\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 2, Test Loss: 2.1233158111572266, Test Accuracy: 22.974916458129883\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.099303\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.025007\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.077864\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.069657\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.021791\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.033797\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.229841\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.244318\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.872572\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 2.046327\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 2.270578\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 2.016361\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 2.040877\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.931247\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 2.142066\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 2.105261\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 3, Test Loss: 2.0326502323150635, Test Accuracy: 26.922285079956055\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 3, Test Loss: 2.027036666870117, Test Accuracy: 27.8782901763916\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.041773\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.904091\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.133851\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.141375\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.785530\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.094611\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.983326\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.031622\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.922186\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 2.019615\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.822431\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 2.103576\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.978317\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 2.130027\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 2.081499\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 2.087636\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 4, Test Loss: 1.9011749029159546, Test Accuracy: 32.103206634521484\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 4, Test Loss: 1.9211126565933228, Test Accuracy: 31.31167984008789\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.885643\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.782487\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.893833\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.123198\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.957809\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.770075\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.833211\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.902623\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.687488\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.629015\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.757722\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.766855\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.665014\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.729818\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.670227\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.772067\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 5, Test Loss: 1.751670479774475, Test Accuracy: 36.56455612182617\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 5, Test Loss: 1.7525696754455566, Test Accuracy: 36.56455612182617\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.864682\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.784308\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.760490[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.632964\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.739670\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.868035\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.472924\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.541255\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.818210\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.794230\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.651877\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.598627\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.620223\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.601664\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.771624\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.608590\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 6, Test Loss: 1.6551048755645752, Test Accuracy: 39.28865051269531\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 6, Test Loss: 1.6598633527755737, Test Accuracy: 39.967105865478516\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.665131\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.903225\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.652824\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.529681\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.831601\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.665424\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.638394\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.574575\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.562814\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.594102\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.480721\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.725146\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.724752\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.738236\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.820101\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.866295\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 7, Test Loss: 1.6227459907531738, Test Accuracy: 41.39596939086914\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 7, Test Loss: 1.6195341348648071, Test Accuracy: 42.12582015991211\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.580948\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.604964\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.515781\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.444819\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.503605\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.696898\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.639914\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.659090\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.653805\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.674321\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.442172\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.645751\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.731404\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.616872\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.568382\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.566187\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 8, Test Loss: 1.5704282522201538, Test Accuracy: 42.94818878173828[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 8, Test Loss: 1.5681519508361816, Test Accuracy: 43.25657653808594[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.625412\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.600089\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.492597\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.586233\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.506921\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.426822\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.501578\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.422035\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.764488\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.666422\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.689980\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.399004\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.720989\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.463376\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.669120\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.440182\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 9, Test Loss: 1.5310451984405518, Test Accuracy: 45.08634948730469[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 9, Test Loss: 1.5361665487289429, Test Accuracy: 45.51809310913086\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.427869\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.571453\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.494863[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.545152\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.378921\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.352516\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.487910\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.623325\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.514193\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.497938\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.660220\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.568584\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.582657\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.362196\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.326334\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.343249\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 10, Test Loss: 1.4917914867401123, Test Accuracy: 46.63856887817383[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 10, Test Loss: 1.4842005968093872, Test Accuracy: 46.998355865478516\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.445983\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.731737\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.488047\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.626533\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.725547\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.453196\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.615815\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.143134\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.666346\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.214780\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.677402\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.382619\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.472114\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.513728\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.040543\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.620785\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 11, Test Loss: 1.472733497619629, Test Accuracy: 48.22162628173828[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 11, Test Loss: 1.4640825986862183, Test Accuracy: 48.488895416259766[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.471933\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.659415\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.227753\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.636158\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.456603\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.360206\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.192881\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.408175\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.303791\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.322789\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.481780\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.577738\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.319700\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.583829\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.504978\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.240527\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 12, Test Loss: 1.4672664403915405, Test Accuracy: 48.129112243652344\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 12, Test Loss: 1.475901484489441, Test Accuracy: 48.324424743652344\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.330616\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.456000\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.501537\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.417813\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.509898\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.426907\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.233477\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.420852\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.539379\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.533051\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.475181\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.375775\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.367397\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.432896\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.373889\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.212075\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 13, Test Loss: 1.4107192754745483, Test Accuracy: 50.339229583740234\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 13, Test Loss: 1.408267617225647, Test Accuracy: 50.59621810913086\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.431272\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.333158\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.409406\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.335835\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.381697\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.268714\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.274051\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.174529\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.462843\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.577206\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.404798\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.425298\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.309991\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.223791\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.398169\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.219211\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 14, Test Loss: 1.409264087677002, Test Accuracy: 50.339229583740234\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 14, Test Loss: 1.4009947776794434, Test Accuracy: 51.058799743652344[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.196905\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.474588\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.304540\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.501179\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.171320\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.250921\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.205852\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.230310\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.510477\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.413298\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.190303\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.533431\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.226655\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.450150\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.078065\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.148481\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 15, Test Loss: 1.3417829275131226, Test Accuracy: 52.6315803527832\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 15, Test Loss: 1.3221641778945923, Test Accuracy: 53.28947448730469[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.435494\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.761966\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.221969\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.277538\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.362882\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.414786\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.313434\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.254628\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.222239\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.978600\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.159901\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.104688\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.326311\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.385922\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.351189\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.153144\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 16, Test Loss: 1.3657726049423218, Test Accuracy: 52.425987243652344\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 16, Test Loss: 1.354589819908142, Test Accuracy: 52.55961990356445\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.125359\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.213598\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.395452\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.506869\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.546988\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.241620\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.495817\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.509465\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.282579\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.547187\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.239686\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.277223\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.429009\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.578753\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.104453\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.261924\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 17, Test Loss: 1.2703496217727661, Test Accuracy: 55.386512756347656\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 17, Test Loss: 1.2781602144241333, Test Accuracy: 54.75945281982422\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.100178\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.182212\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.164321\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.234389\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.025947\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.255785\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.055330\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.174731\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.317120\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.170118\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 0.949948\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.098088\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.131822\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.118162\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.073510\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.152310\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 18, Test Loss: 1.2673500776290894, Test Accuracy: 55.65378189086914\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 18, Test Loss: 1.2697452306747437, Test Accuracy: 55.44819259643555[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.160249\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.177958\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.340107\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.121762\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.355179\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.444639\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.502374\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.380760\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.211604\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.030225\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.988776\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.289516\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.052789\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.211738\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.179052\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.161950\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 19, Test Loss: 1.235142707824707, Test Accuracy: 57.24711990356445\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 19, Test Loss: 1.2418733835220337, Test Accuracy: 56.56867218017578\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.139738\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.259161\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.298053\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.270063\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.264603\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.154275\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.189095\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.355678\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.144371\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.528025\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.147041\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.154138\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.487721\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.261875\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.981274\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.336862\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 20, Test Loss: 1.200790286064148, Test Accuracy: 57.96669006347656[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 20, Test Loss: 1.2227591276168823, Test Accuracy: 57.7199821472168\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.250108\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.178230\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.227786\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.120519\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.409216\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.012179\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.106636\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.419915\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.390642\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.067787\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.323555\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.251665\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.216500\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.135244\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.271963\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.149977\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 21, Test Loss: 1.2721757888793945, Test Accuracy: 56.59950256347656\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 21, Test Loss: 1.2575101852416992, Test Accuracy: 57.44243240356445\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.175869\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.384754\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.291646\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.153766\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.159592\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.248899\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.016201\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.152176\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.903713\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.149102\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.228393\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.270083\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.063218\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.098841\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.957229\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.210573\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 22, Test Loss: 1.1619467735290527, Test Accuracy: 60.00205993652344\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 22, Test Loss: 1.1663093566894531, Test Accuracy: 59.95065689086914\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.004940\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.048088\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.085320\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.952558\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.101950\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.233153\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.306147\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.132257\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.231845\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.165174\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.130381\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.010732\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.292676\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.148911\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.039912\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.129272\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 23, Test Loss: 1.1652841567993164, Test Accuracy: 60.13569259643555\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 23, Test Loss: 1.1633505821228027, Test Accuracy: 59.94038009643555\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.185715\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.022943\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.323882\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.247439\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.314649\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 0.992885\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.282108\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.131988\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.020899\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.970387\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.192693\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.954959\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.240591\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.018870\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.193832\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.025196\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 24, Test Loss: 1.153822660446167, Test Accuracy: 60.074012756347656[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 24, Test Loss: 1.1529427766799927, Test Accuracy: 60.50575256347656\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.194684\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.080969\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.929007\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.014086\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.093929\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.962421\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.054419\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.038181\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.819725\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.174421\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.237905\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.185539\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.026446\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 0.912548\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.224291\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.230065\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 25, Test Loss: 1.1486937999725342, Test Accuracy: 60.08429718017578[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 25, Test Loss: 1.1524332761764526, Test Accuracy: 60.14596939086914\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.027808\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.331281\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.300182\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.869528\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.230514\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.238069\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.298500\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.085235\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.352937\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.946480\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.287254\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.039051\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.090942\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 0.946694\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.055989\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.093805\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 26, Test Loss: 1.102002501487732, Test Accuracy: 61.70846939086914\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 26, Test Loss: 1.1052956581115723, Test Accuracy: 61.49259948730469[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.134508\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.028290\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.874814\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.373035\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.182923\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.053310\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.954997\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 0.928540\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.170612\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.894056\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.204872\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.017515\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 0.938357\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 0.991734\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.109722\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.998323\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 27, Test Loss: 1.1278949975967407, Test Accuracy: 61.019737243652344\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 27, Test Loss: 1.1382137537002563, Test Accuracy: 61.16365051269531[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.082186\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.005877\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.259221\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.187546\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.149942\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.172037\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.125376\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.140369\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.887577\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.935939\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.096990\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 0.982882\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.904368\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.021620\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.085708\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.181554[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 28, Test Loss: 1.0707348585128784, Test Accuracy: 62.95230484008789\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 28, Test Loss: 1.0768219232559204, Test Accuracy: 62.643917083740234\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.385717\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.125105\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.987363\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.064467\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 0.916763\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 0.967707\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 0.835703\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.994817\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.874947\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.838913\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.152417\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 0.919691\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.175608\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.130553\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.165418\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.020022\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 29, Test Loss: 1.0731383562088013, Test Accuracy: 62.92146301269531\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 29, Test Loss: 1.083306074142456, Test Accuracy: 62.42804718017578\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.251722\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.943468\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.005983\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.081895\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.102589\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.248761\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 0.899434\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.128802\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.868185\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.013671\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.112689\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.063008\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.120981\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.118360\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.913107\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.061134\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 30, Test Loss: 1.0823590755462646, Test Accuracy: 62.777549743652344[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 30, Test Loss: 1.0832382440567017, Test Accuracy: 62.71586990356445\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.029835\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.935976\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.241500\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.009961\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.249708\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.013464\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.143084\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.900707\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.782159\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.227900\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 1.044300\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 0.889162\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.988462\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.076339\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 0.883989\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.068028\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 31, Test Loss: 1.0714362859725952, Test Accuracy: 63.5793571472168\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 31, Test Loss: 1.0652124881744385, Test Accuracy: 63.61019515991211\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.186581\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.969694\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.041646\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.937792\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.077902\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.126593\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 0.733919\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.137556\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.823889\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.898229\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.118409\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.126098\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.237299\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.161007\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.034945\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.914724\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 32, Test Loss: 1.0342556238174438, Test Accuracy: 64.43256378173828\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 32, Test Loss: 1.030648946762085, Test Accuracy: 64.68955993652344\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.849970\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 0.957731\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.111511\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.829340\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.933010\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 0.950762\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 0.902207\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.103966\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.905145\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.961455\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.021156\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.178745\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 0.840285\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.922180\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.034694\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 0.959378\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 33, Test Loss: 1.0131068229675293, Test Accuracy: 65.26521301269531\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 33, Test Loss: 1.0139576196670532, Test Accuracy: 65.09046173095703\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.159127\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.275918\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.226762\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.097171\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.024767\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.809904\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.037584\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.398510\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.004122\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.793041\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.036735\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.063032\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.001412\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.945563\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 0.804991\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.863998\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 34, Test Loss: 1.01542067527771, Test Accuracy: 65.25493621826172\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 34, Test Loss: 1.0137828588485718, Test Accuracy: 65.08018493652344\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.817302\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 0.988864\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 0.884430\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.927218\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 0.673419\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.845359\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.004497\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.117816\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.837384\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.820686\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.817625\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 0.939055\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.797193\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 1.019911\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.097476\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.052195\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 35, Test Loss: 1.008974552154541, Test Accuracy: 65.14185333251953[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 35, Test Loss: 1.0040777921676636, Test Accuracy: 64.67927551269531[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.305629\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.111905\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.844505\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.954187\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.126138\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 0.936436\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 0.869974\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.893665\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.251304\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.806912\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 1.013536\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.029450\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 0.920153\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.118531\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.838343\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.941485\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 36, Test Loss: 1.023807406425476, Test Accuracy: 64.99794006347656\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 36, Test Loss: 1.008224368095398, Test Accuracy: 65.16242218017578\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 0.939647\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.808478\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.019212\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.822939\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.956438\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 0.934908\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.029263\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.055937\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.704953\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.763266\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 0.768905\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.016402\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.832963\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 0.965297\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.951462\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 0.746713\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 37, Test Loss: 0.9721676707267761, Test Accuracy: 66.25205993652344[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 37, Test Loss: 0.9781517386436462, Test Accuracy: 66.33429718017578\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.605006\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 0.900531\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 0.804601\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 0.747712\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.849505\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 0.712208\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.885308\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.056325\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.983379\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.917066\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.902650\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 0.813897\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 0.854195\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.920401\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.894101\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 0.978515\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 38, Test Loss: 0.9692111611366272, Test Accuracy: 66.74547576904297\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 38, Test Loss: 0.9624542593955994, Test Accuracy: 66.83799743652344\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.018206\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.838559\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 0.841687\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 0.850645\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 0.733624\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 0.919840\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 0.840098\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.830713\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.922815\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 0.983371\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 0.886277\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.821172\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 0.976975\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 0.773215\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 1.104024\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.961392\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 39, Test Loss: 0.9797499775886536, Test Accuracy: 66.015625\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 39, Test Loss: 0.9912800192832947, Test Accuracy: 66.05674743652344\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 0.794351\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 0.889301\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 0.927890\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.031384\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 0.838343\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.950760\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 0.897457\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 0.895751\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #50#011Loss: 0.829587\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #50#011Loss: 1.090580\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #50#011Loss: 0.926558\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #50#011Loss: 1.236829\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #50#011Loss: 1.118476\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #50#011Loss: 0.714120\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #50#011Loss: 0.879651\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #50#011Loss: 1.048547\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 40, Test Loss: 0.9645495414733887, Test Accuracy: 66.83799743652344\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Saving Model  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model is saved in /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## Epoch 40, Test Loss: 0.9825441241264343, Test Accuracy: 66.42681121826172\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Saving Model  ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Model is saved in /opt/ml/model\u001b[0m\n",
      "\u001b[35m2021-10-10 05:14:36,193 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.2.127.148' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-10 05:10:21.368429: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-10-10 05:10:21.368449: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-10-10 05:10:21.368434: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-10-10 05:10:21.368486: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-10-10 05:10:21.368586: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-10 05:10:21.368585: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-10-10 05:10:21.368587: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-10-10 05:10:21.368622: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-10-10 05:10:21.385215: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-10-10 05:10:21.385209: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-10-10 05:10:21.385210: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-10 05:10:21.385270: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-10-10 05:10:21.385380: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-10-10 05:10:21.385382: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-10-10 05:10:21.385382: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-10 05:10:21.385400: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-10-10 05:10:21.408995: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-10-10 05:10:21.408968: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-10 05:10:21.408968: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-10-10 05:10:21.409308: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-10-10 05:10:21.426040: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-10 05:10:21.426109: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-10-10 05:10:21.426422: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-10-10 05:10:21.426570: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-10 05:14:34.192355: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-10 05:14:34.210550: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-10 05:14:36,184 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2021-10-10 05:15:06,224 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:15:06,224 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-10-10 05:16:04 Uploading - Uploading generated training model\n",
      "2021-10-10 05:16:04 Completed - Training job completed\n",
      "ProfilerReport-1633842236: IssuesFound\n",
      "Training seconds: 1064\n",
      "Billable seconds: 1064\n"
     ]
    }
   ],
   "source": [
    "horovod_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 정리 작업\n",
    "\n",
    "## 모델 아티펙트 저장\n",
    "- S3 에 저장된 모델 아티펙트를 저장하여 추론시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horovod_artifact_path:  s3://sagemaker-us-east-1-057716757052/cifar10-tf-dist-2021-10-10-05-03-55-991/output/model.tar.gz\n",
      "Stored 'tf2_horovod_artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "tf2_horovod_artifact_path = horovod_estimator.model_data\n",
    "print(\"horovod_artifact_path: \", tf2_horovod_artifact_path)\n",
    "\n",
    "\n",
    "%store tf2_horovod_artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-10 05:15:47   12060734 cifar10-tf-dist-2021-10-10-05-03-55-991/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {tf2_horovod_artifact_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
