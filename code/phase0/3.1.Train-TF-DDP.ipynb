{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 3.1] SageMaker DDP 모델 훈련\n",
    "\n",
    "\n",
    "### 중요 사항\n",
    "\n",
    "- 이 예시는 노트북 인스턴스가 **<font color=\"red\">ml.p3.16xlarge</font>** 에서만 동작 합니다.\n",
    "- 본 워크샵의 모든 노트북은 **<font color=\"red\">conda_tensorflow2_p36</font>** 를 사용합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 1. 기본 환경 세팅 \n",
    "- 2. 데이터 세트를 S3 에 업로딩\n",
    "- 3. 노트북에서 세이지 메이커 스크립트 모드 스타일로 코드 변경\n",
    "- 4. 세이지 메이커 로컬 모드로 훈련\n",
    "- 5. 세이지 메이커의 호스트 모드로 훈련\n",
    "- 6. 모델 아티펙트 경로 저장\n",
    "\n",
    "## 참고:\n",
    "\n",
    "- 세이지 메이커의 공식 개발자 가이드 입니다.\n",
    "    - [개발자 가이드](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html)\n",
    "\n",
    "\n",
    "- 세이지 메이커 분산 라이브러리 예세 Git 입니다.\n",
    "    - [세이지 메이커 분산 라이브러리 공식 예제](https://github.com/aws/amazon-sagemaker-examples/tree/master/training/distributed_training)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-pytorch-cnn-cifar10\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local_gpu\"\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_dir\n",
    "%store -r validation_dir\n",
    "%store -r eval_dir\n",
    "%store -r data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 세트를 S3에 업로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-057716757052/data/DEMO-cifar10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_location = sagemaker_session.upload_data(path=data_dir, key_prefix='data/DEMO-cifar10')\n",
    "display(dataset_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 훈련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local_gpu\"\n",
    "# instance_type = \"ml.p3.8xlarge\"\n",
    "\n",
    "job_name ='cifar10-horovod'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시스템의 이전 도커 컨테이너 삭제\n",
    "- 아래와 같은 명령어를 사용하여 저장 공간을 확보 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도커 컨테이너 모두 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs        241G   80K  241G   1% /dev\n",
      "tmpfs           241G     0  241G   0% /dev/shm\n",
      "/dev/xvda1      109G  108G  863M 100% /\n",
      "/dev/xvdf       984G   25G  910G   3% /home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "! df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! docker container prune -f \n",
    "# ! rm -rf /tmp/tmp*\n",
    "# ! df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도커 이미지 모두 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! df -h\n",
    "# ! docker image prune -f --all\n",
    "# ! df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 용량 확보\n",
    "\n",
    "추가적인 용량 삭제가 필요하면 아래를 실행 하세요\n",
    "```\n",
    "rm -rf /tmp/tmp*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 로컬모드로 훈련\n",
    "- 현 실행 노트북 인스턴스에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_gpu_learning_rate:  0.000125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count ):\n",
    "    total_gpu = num_gpu * train_instance_count\n",
    "\n",
    "    multi_gpu_learning_rate = one_gpu_learning_rate / total_gpu\n",
    "    print(\"multi_gpu_learning_rate: \", multi_gpu_learning_rate)\n",
    "    \n",
    "    return multi_gpu_learning_rate\n",
    "\n",
    "train_instance_type = 'ml.p3.16xlarge'\n",
    "num_gpu = 8\n",
    "train_instance_count = 1\n",
    "one_gpu_learning_rate = 0.001 \n",
    "\n",
    "multi_gpu_learning_rate = calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "                    'epochs' : 10,\n",
    "                    'learning-rate' : f\"{multi_gpu_learning_rate}\",\n",
    "                    'print-interval' : 100,\n",
    "                    'train-batch-size': 64,    \n",
    "                    'eval-batch-size': 512,        \n",
    "                    'validation-batch-size': 512,\n",
    "                  }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating le2gyt8gwa-algo-1-dopi5 ... \n",
      "Creating le2gyt8gwa-algo-1-dopi5 ... done\n",
      "Attaching to le2gyt8gwa-algo-1-dopi5\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:26.677031: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:26.677227: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:26.681562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:26.718624: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:28,545 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:29,030 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:29,031 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:29,039 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:29,039 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:29,039 sagemaker-training-toolkit INFO     Host: ['algo-1-dopi5']\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:29,041 sagemaker-training-toolkit INFO     instance type: local_gpu\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:26:29,131 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m Training Env:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m {\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"additional_framework_parameters\": {\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"sagemaker_instance_type\": \"local_gpu\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\"\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     },\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"validation\": \"/opt/ml/input/data/validation\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"eval\": \"/opt/ml/input/data/eval\"\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     },\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"current_host\": \"algo-1-dopi5\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"algo-1-dopi5\"\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     ],\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"epochs\": 10,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"learning-rate\": \"0.000125\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"print-interval\": 100,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"train-batch-size\": 64,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"eval-batch-size\": 512,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"validation-batch-size\": 512,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"model_dir\": \"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/model\"\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     },\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"train\": {\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         },\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"validation\": {\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         },\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"eval\": {\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         }\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     },\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"job_name\": \"cifar10-sm-ddp-2021-10-10-05-26-20-080\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"master_hostname\": \"algo-1-dopi5\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/source/sourcedir.tar.gz\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"module_name\": \"cifar10_tf2_sm_ddp\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"current_host\": \"algo-1-dopi5\",\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m             \"algo-1-dopi5\"\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m         ]\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     },\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m     \"user_entry_point\": \"cifar10_tf2_sm_ddp.py\"\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m }\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m Environment variables:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_HOSTS=[\"algo-1-dopi5\"]\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_HPS={\"epochs\":10,\"eval-batch-size\":512,\"learning-rate\":\"0.000125\",\"model_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/model\",\"print-interval\":100,\"train-batch-size\":64,\"validation-batch-size\":512}\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_USER_ENTRY_POINT=cifar10_tf2_sm_ddp.py\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"local_gpu\"}\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-dopi5\",\"hosts\":[\"algo-1-dopi5\"]}\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_INPUT_DATA_CONFIG={\"eval\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_CHANNELS=[\"eval\",\"train\",\"validation\"]\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_CURRENT_HOST=algo-1-dopi5\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_MODULE_NAME=cifar10_tf2_sm_ddp\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/source/sourcedir.tar.gz\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"local_gpu\"},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1-dopi5\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-dopi5\"],\"hyperparameters\":{\"epochs\":10,\"eval-batch-size\":512,\"learning-rate\":\"0.000125\",\"model_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/model\",\"print-interval\":100,\"train-batch-size\":64,\"validation-batch-size\":512},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"},\"validation\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-sm-ddp-2021-10-10-05-26-20-080\",\"log_level\":20,\"master_hostname\":\"algo-1-dopi5\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_tf2_sm_ddp\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-dopi5\",\"hosts\":[\"algo-1-dopi5\"]},\"user_entry_point\":\"cifar10_tf2_sm_ddp.py\"}\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"10\",\"--eval-batch-size\",\"512\",\"--learning-rate\",\"0.000125\",\"--model_dir\",\"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/model\",\"--print-interval\",\"100\",\"--train-batch-size\",\"64\",\"--validation-batch-size\",\"512\"]\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_CHANNEL_EVAL=/opt/ml/input/data/eval\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_HP_EPOCHS=10\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_HP_LEARNING-RATE=0.000125\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_HP_PRINT-INTERVAL=100\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_HP_TRAIN-BATCH-SIZE=64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_HP_EVAL-BATCH-SIZE=512\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_HP_VALIDATION-BATCH-SIZE=512\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m SM_HP_MODEL_DIR=s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/model\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m mpirun --host algo-1-dopi5 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so smddprun /usr/local/bin/python3.7 -m mpi4py cifar10_tf2_sm_ddp.py --epochs 10 --eval-batch-size 512 --learning-rate 0.000125 --model_dir s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/model --print-interval 100 --train-batch-size 64 --validation-batch-size 512\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:tensorflow version:  2.4.1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Using network Socket\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:NCCL version 2.7.8+cuda11.0\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Bootstrap : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Using network Socket\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Using network Socket\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Using network Socket\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Using network Socket\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO NET/IB : No device found.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Using network Socket\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Using network Socket\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO NET/Socket : Using [0]eth0:172.18.0.2<0>\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Using network Socket\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO comm 0x5585e6411410 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO comm 0x5637b6217300 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO comm 0x5586ccefed60 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO comm 0x561057dba940 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO comm 0x5601becb9f50 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO comm 0x55d051b251c0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO comm 0x562025cf8900 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO comm 0x5643c913d8f0 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:algo-1-dopi5:163:163 [1] NCCL INFO comm 0x5585e90e74f0 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:618 [0] NCCL INFO comm 0x5637b8eed3e0 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:algo-1-dopi5:165:165 [2] NCCL INFO comm 0x5586cfbd4e40 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:algo-1-dopi5:167:167 [3] NCCL INFO comm 0x56105aa90a20 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:algo-1-dopi5:168:168 [4] NCCL INFO comm 0x5601c1990030 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:algo-1-dopi5:170:170 [5] NCCL INFO comm 0x55d0547faf40 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:algo-1-dopi5:171:171 [6] NCCL INFO comm 0x5620289ce9e0 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:algo-1-dopi5:172:172 [7] NCCL INFO comm 0x5643cbe13820 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:Running smdistributed.dataparallel v1.2.0\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## dist.rank():  0\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## args: \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>: Namespace(epochs=10, eval='/opt/ml/input/data/eval', eval_batch_size=512, learning_rate=0.000125, model_dir='s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-26-20-080/model', model_output_dir='/opt/ml/model', momentum=0.9, optimizer='adam', print_interval=100, train='/opt/ml/input/data/train', train_batch_size=64, validation='/opt/ml/input/data/validation', validation_batch_size=512, weight_decay=0.0002)\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:################# Loading Dataset ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:Channel Name: train\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:# of batches loading TFRecord : 40000\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:buffer_size:  40000\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:################# Loading Dataset ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:Channel Name: eval\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:# of batches loading TFRecord : 10000\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:################# Loading Dataset ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:Channel Name: validation\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:# of batches loading TFRecord : 10000\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## num_train_batch on each GPU6 : 78 \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:################# Prepare Dataset ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:# of batches in train:  625\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:# of batches in eval:  19\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:# of batches in validation:  19\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## num_train_batch on each GPU2 : 78 \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## num_train_batch on each GPU0 : 78 [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## num_train_batch on each GPU1 : 78 \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## num_train_batch on each GPU4 : 78 \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## num_train_batch on each GPU3 : 78 \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## num_train_batch on each GPU5 : 78 \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## num_train_batch on each GPU7 : 78 \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:[2021-10-10 05:26:55.438 algo-1-dopi5:171 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:[2021-10-10 05:26:55.441 algo-1-dopi5:618 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:[2021-10-10 05:26:55.444 algo-1-dopi5:165 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:[2021-10-10 05:26:55.471 algo-1-dopi5:171 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:[2021-10-10 05:26:55.475 algo-1-dopi5:618 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:[2021-10-10 05:26:55.477 algo-1-dopi5:165 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:[2021-10-10 05:26:55.604 algo-1-dopi5:167 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:[2021-10-10 05:26:55.627 algo-1-dopi5:168 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:[2021-10-10 05:26:55.640 algo-1-dopi5:167 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:[2021-10-10 05:26:55.645 algo-1-dopi5:163 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:[2021-10-10 05:26:55.660 algo-1-dopi5:168 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:[2021-10-10 05:26:55.681 algo-1-dopi5:163 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:[2021-10-10 05:26:55.691 algo-1-dopi5:170 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:[2021-10-10 05:26:55.715 algo-1-dopi5:172 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:[2021-10-10 05:26:55.725 algo-1-dopi5:170 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:[2021-10-10 05:26:55.750 algo-1-dopi5:172 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:algo-1-dopi5:618:1318 [0] NCCL INFO Launch mode Parallel\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 44.661880\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 44.648071\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 43.970490\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 43.627914\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 43.688923\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 42.175591[1,6]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 44.309738\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 45.498100\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 1, Test Loss: 2.266072988510132, Test Accuracy: 15.059621810913086\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 2.256430\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 2.311102\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 2.290498\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 2.261552\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 2.269154\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 2.270797\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 2.282504\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 2.290916\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 2, Test Loss: 2.08024263381958, Test Accuracy: 23.077714920043945\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.935266\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 2.000124\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.958111\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.926864\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 2.033978\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.986092\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 2.088050\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.964758\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 3, Test Loss: 2.008030652999878, Test Accuracy: 26.03824234008789[1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.857491\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.971777\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.863704\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.888924\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.931776\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.923352\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.903484\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.959095\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 4, Test Loss: 2.0015227794647217, Test Accuracy: 29.923931121826172\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 2.022468\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 2.038075\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.987457\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 2.160629\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 2.068569\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 2.029011\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.954455\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 2.088557\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 5, Test Loss: 1.8617497682571411, Test Accuracy: 32.01068878173828\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.968999\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 2.092624\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.965099\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 2.066765\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 2.040934\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.975735\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.982942\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 2.019363\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 6, Test Loss: 1.8223448991775513, Test Accuracy: 33.871299743652344\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.739530\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.833252\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.798999\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.825888\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.751885\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.778882\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.895256\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.801488\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 7, Test Loss: 1.724145531654358, Test Accuracy: 37.037418365478516\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.767446\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.759742\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.673213\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.672369\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.754493\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.660943\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.728474\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.527136\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 8, Test Loss: 1.6693156957626343, Test Accuracy: 39.24753189086914\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.663752\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.627569\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.590999\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.698978\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.696247[1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.768709\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.794271\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.652607\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 9, Test Loss: 1.680158257484436, Test Accuracy: 38.88774871826172\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## GPU0 - Step #0\tLoss: 1.660681\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:## GPU7 - Step #0\tLoss: 1.682280\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:## GPU4 - Step #0\tLoss: 1.754107\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:## GPU1 - Step #0\tLoss: 1.619873\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:## GPU6 - Step #0\tLoss: 1.668634\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:## GPU5 - Step #0\tLoss: 1.573453\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:## GPU2 - Step #0\tLoss: 1.647974\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:## GPU3 - Step #0\tLoss: 1.812053\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:Training Finished.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:Training Finished.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:Training Finished.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:Training Finished.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:Training Finished.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:Training Finished.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:Training Finished.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:## Epoch 10, Test Loss: 1.6754666566848755, Test Accuracy: 40.75863265991211\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:Training Finished.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:################# Start Training  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:################# Saving Model  ################\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stdout>:Model is saved in /opt/ml/model\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stderr>:2021-10-10 05:26:29.381947: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stderr>:2021-10-10 05:26:29.381944: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stderr>:2021-10-10 05:26:29.382103: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stderr>:2021-10-10 05:26:29.381944: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stderr>:2021-10-10 05:26:29.382104: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stderr>:2021-10-10 05:26:29.381944: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stderr>:2021-10-10 05:26:29.382103: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stderr>:2021-10-10 05:26:29.382103: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stderr>:2021-10-10 05:26:29.391240: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stderr>:2021-10-10 05:26:29.391242: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stderr>:2021-10-10 05:26:29.391243: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stderr>:2021-10-10 05:26:29.391417: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stderr>:2021-10-10 05:26:29.391417: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stderr>:2021-10-10 05:26:29.391417: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,5]<stderr>:2021-10-10 05:26:29.422894: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,3]<stderr>:2021-10-10 05:26:29.422894: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,1]<stderr>:2021-10-10 05:26:29.423372: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,2]<stderr>:2021-10-10 05:26:29.423570: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,7]<stderr>:2021-10-10 05:26:29.432376: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,6]<stderr>:2021-10-10 05:26:29.432559: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,4]<stderr>:2021-10-10 05:26:29.432820: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stderr>:2021-10-10 05:26:30.169438: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stderr>:2021-10-10 05:26:30.169608: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stderr>:2021-10-10 05:26:30.211322: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stderr>:2021-10-10 05:27:39.797917: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m [1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m \n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 |\u001b[0m 2021-10-10 05:27:43,196 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mle2gyt8gwa-algo-1-dopi5 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "job_name ='cifar10-sm-ddp'\n",
    "estimator = TensorFlow(base_job_name= job_name,\n",
    "                       entry_point='cifar10_tf2_sm_ddp.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       framework_version='2.4.1',\n",
    "                       py_version='py37',\n",
    "                       script_mode=True,                            \n",
    "                       hyperparameters= hyperparameters,\n",
    "                       train_instance_count=1,   # 변경\n",
    "                       train_instance_type='local_gpu',\n",
    "                       distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    "                       debugger_hook_config=False,                       \n",
    "                      )\n",
    "\n",
    "\n",
    "estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬모드에서 도커 이미지 다운로드 된 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                                         TAG                 IMAGE ID            CREATED             SIZE\n",
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training   2.4.1-gpu-py37      8467bc1c5070        5 months ago        8.91GB\n"
     ]
    }
   ],
   "source": [
    "! docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 호스트 모드로 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi_gpu_learning_rate\n",
    "- GPU 의 개수, Batch Size, Epoch 당 배치 수 에 따라 튜닝이 필요한 수치 입니다. 여기서는 예시로 사용한 것이기에, 실제 사용시에 적절하게 튜닝을 해주시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_gpu_learning_rate:  6.25e-05\n"
     ]
    }
   ],
   "source": [
    "train_instance_type = 'ml.p3.16xlarge'\n",
    "num_gpu = 8\n",
    "train_instance_count = 2\n",
    "one_gpu_learning_rate = 0.001 \n",
    "\n",
    "multi_gpu_learning_rate = calculate_learning_rate(one_gpu_learning_rate, num_gpu, train_instance_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "hyperparameters = {\n",
    "                    'epochs' : 50,\n",
    "                    'learning-rate' : f\"{multi_gpu_learning_rate}\",\n",
    "                    'print-interval' : 100,\n",
    "                    'train-batch-size': 64,    \n",
    "                    'eval-batch-size': 512,        \n",
    "                    'validation-batch-size': 512,\n",
    "                  }\n",
    "\n",
    "\n",
    "job_name ='cifar10-sm-ddp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ddp_estimator = TensorFlow(base_job_name= job_name,\n",
    "                       entry_point='cifar10_tf2_sm_ddp.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       framework_version='2.4.1',\n",
    "                       py_version='py37',\n",
    "                       script_mode=True,                            \n",
    "                       hyperparameters= hyperparameters,\n",
    "                       train_instance_count=train_instance_count,   # 변경\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    "                       debugger_hook_config=False,                       \n",
    "                      )\n",
    "\n",
    "\n",
    "ddp_estimator.fit({'train':'{}/train'.format(dataset_location),\n",
    "              'validation':'{}/validation'.format(dataset_location),\n",
    "              'eval':'{}/eval'.format(dataset_location)}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-10 05:27:45 Starting - Starting the training job...\n",
      "2021-10-10 05:27:48 Starting - Launching requested ML instancesProfilerReport-1633843665: InProgress\n",
      ".........\n",
      "2021-10-10 05:29:38 Starting - Preparing the instances for training.........\n",
      "2021-10-10 05:31:06 Downloading - Downloading input data...\n",
      "2021-10-10 05:31:39 Training - Downloading the training image..............\u001b[34m2021-10-10 05:33:57.516542: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:33:57.523293: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:33:57.641516: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-10-10 05:33:57.760163: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:01,991 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\n",
      "2021-10-10 05:34:20 Training - Training image download completed. Training in progress.\u001b[34m2021-10-10 05:34:07.161161: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:07.165720: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:07.253622: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:07.350145: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:09,030 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:09,030 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:09,032 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:09,032 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.242.34\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:10,034 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:10,034 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.242.34\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:11,036 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:11,036 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.242.34\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:10,985 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:11,666 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:11,666 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:11,675 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:11,677 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:11,678 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:12,045 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:12,122 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:12,122 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:12,123 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:12,123 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:12,133 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:12,687 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:12,760 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:12,761 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:12,761 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:12,761 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:12,761 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:12,761 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:12,762 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2021-10-10 05:34:12,852 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"eval\": \"/opt/ml/input/data/eval\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"validation-batch-size\": 512,\n",
      "        \"learning-rate\": \"6.25e-05\",\n",
      "        \"print-interval\": 100,\n",
      "        \"model_dir\": \"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/model\",\n",
      "        \"train-batch-size\": 64,\n",
      "        \"epochs\": 50,\n",
      "        \"eval-batch-size\": 512\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"eval\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"cifar10-sm-ddp-2021-10-10-05-27-44-881\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"cifar10_tf2_sm_ddp\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"cifar10_tf2_sm_ddp.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":50,\"eval-batch-size\":512,\"learning-rate\":\"6.25e-05\",\"model_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/model\",\"print-interval\":100,\"train-batch-size\":64,\"validation-batch-size\":512}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=cifar10_tf2_sm_ddp.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"eval\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=cifar10_tf2_sm_ddp\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"eval\":\"/opt/ml/input/data/eval\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":50,\"eval-batch-size\":512,\"learning-rate\":\"6.25e-05\",\"model_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/model\",\"print-interval\":100,\"train-batch-size\":64,\"validation-batch-size\":512},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"eval\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"cifar10-sm-ddp-2021-10-10-05-27-44-881\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/source/sourcedir.tar.gz\",\"module_name\":\"cifar10_tf2_sm_ddp\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"cifar10_tf2_sm_ddp.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"50\",\"--eval-batch-size\",\"512\",\"--learning-rate\",\"6.25e-05\",\"--model_dir\",\"s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/model\",\"--print-interval\",\"100\",\"--train-batch-size\",\"64\",\"--validation-batch-size\",\"512\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_EVAL=/opt/ml/input/data/eval\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION-BATCH-SIZE=512\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=6.25e-05\u001b[0m\n",
      "\u001b[34mSM_HP_PRINT-INTERVAL=100\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/model\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN-BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL-BATCH-SIZE=512\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /usr/local/bin/python3.7 -m mpi4py cifar10_tf2_sm_ddp.py --epochs 50 --eval-batch-size 512 --learning-rate 6.25e-05 --model_dir s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/model --print-interval 100 --train-batch-size 64 --validation-batch-size 512\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:14,138 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=172, name='orted', status='disk-sleep', started='05:34:13')]\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:14,138 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=172, name='orted', status='disk-sleep', started='05:34:13')]\u001b[0m\n",
      "\u001b[35m2021-10-10 05:34:14,139 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=172, name='orted', status='disk-sleep', started='05:34:13')]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tensorflow version:  2.4.1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Bootstrap : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Bootstrap : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Bootstrap : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Bootstrap : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.242.34<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Bootstrap : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Bootstrap : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Bootstrap : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Bootstrap : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] find_ofi_provider:542 NCCL WARN NET/OFI Couldn't find any optimal provider\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO NET/IB : No device found.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.253.10<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO comm 0x55e12a1f8d20 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO comm 0x55e0044da6f0 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO comm 0x55d36e091780 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO comm 0x55e101b84f90 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO comm 0x559f01970770 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO comm 0x5649b8f6dbf0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO comm 0x55d690cb2d90 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO comm 0x5558a63eacc0 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO comm 0x55bd068d5e00 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO comm 0x56329979e5c0 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO comm 0x5569a2622ce0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO comm 0x55e53a2130b0 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO comm 0x5589379f74c0 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO comm 0x55eb4a4d0050 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO comm 0x55989e69e640 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO comm 0x55e1486a78e0 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Trees [0] 2/8/-1->3->0|0->3->2/8/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->11|11->0->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Trees [0] 12/-1/-1->15->14|14->15->12/-1/-1 [1] 12/-1/-1->15->14|14->15->12/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Trees [0] -1/-1/-1->12->15|15->12->-1/-1/-1 [1] -1/-1/-1->12->15|15->12->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Trees [0] 14/-1/-1->13->9|9->13->14/-1/-1 [1] 14/-1/-1->13->9|9->13->14/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Trees [0] 15/-1/-1->14->13|13->14->15/-1/-1 [1] 15/-1/-1->14->13|13->14->15/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Trees [0] 11/-1/-1->8->3|3->8->11/-1/-1 [1] 11/-1/-1->8->-1|-1->8->11/-1/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Trees [0] 13/-1/-1->9->10|10->9->13/-1/-1 [1] 13/-1/-1->9->10|10->9->13/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Trees [0] 10/-1/-1->11->8|8->11->10/-1/-1 [1] 10/0/-1->11->8|8->11->10/0/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Trees [0] 9/-1/-1->10->11|11->10->9/-1/-1 [1] 9/-1/-1->10->11|11->10->9/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 00 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 00 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 00 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 00 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 00 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 00 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 00 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 00 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 00 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 00 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 01 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 01 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 01 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 01 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO Channel 01 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO Channel 01 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO Channel 01 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:214:214 [6] NCCL INFO comm 0x55d693988f70 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:216:216 [5] NCCL INFO comm 0x5649bbc43dd0 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:229:229 [5] NCCL INFO comm 0x5598a1374820 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:227:227 [6] NCCL INFO comm 0x55eb4d1a6230 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:223:223 [1] NCCL INFO comm 0x55bd095abfe0 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO Channel 01 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:226:226 [7] NCCL INFO comm 0x55e14b37dac0 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:225:225 [4] NCCL INFO comm 0x55893a6cd6a0 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:219:219 [1] NCCL INFO comm 0x55e12cecef00 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:217:217 [2] NCCL INFO comm 0x55d370d67960 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO Channel 01 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:218:218 [3] NCCL INFO comm 0x559f04646950 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:228:228 [2] NCCL INFO comm 0x5569a52f8ec0 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:220:220 [7] NCCL INFO comm 0x5558a90c0ea0 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:215:215 [4] NCCL INFO comm 0x55e10485b170 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 01 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:678 [0] NCCL INFO comm 0x56329c4747a0 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:669 [0] NCCL INFO comm 0x55e0071b08d0 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:224:224 [3] NCCL INFO comm 0x55e53cee9290 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.2.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## dist.rank():  0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## args: \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Namespace(epochs=50, eval='/opt/ml/input/data/eval', eval_batch_size=512, learning_rate=6.25e-05, model_dir='s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/model', model_output_dir='/opt/ml/model', momentum=0.9, optimizer='adam', print_interval=100, train='/opt/ml/input/data/train', train_batch_size=64, validation='/opt/ml/input/data/validation', validation_batch_size=512, weight_decay=0.0002)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Channel Name: train\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:# of batches loading TFRecord : 40000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Channel Name: train\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches loading TFRecord : 40000\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:buffer_size:  40000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:buffer_size:  40000\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Channel Name: eval\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Channel Name: eval\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## num_train_batch on each GPU10 : 39 \u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Channel Name: validation\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## num_train_batch on each GPU13 : 39 \u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## num_train_batch on each GPU14 : 39 [1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## num_train_batch on each GPU11 : 39 \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Loading Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Channel Name: validation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches loading TFRecord : 10000\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:################# Start Training  ################[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## num_train_batch on each GPU12 : 39 \u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## num_train_batch on each GPU8 : 39 \u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## num_train_batch on each GPU9 : 39 \u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## num_train_batch on each GPU15 : 39 \u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:################# Start Training  ################[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:################# Start Training  ################[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## num_train_batch on each GPU2 : 39 [1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## num_train_batch on each GPU3 : 39 \u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Start Training  ################[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## num_train_batch on each GPU4 : 39 \u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## num_train_batch on each GPU1 : 39 [1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## num_train_batch on each GPU6 : 39 \u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:################# Start Training  ################[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## num_train_batch on each GPU7 : 39 \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Prepare Dataset ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches in train:  625\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches in eval:  19\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:# of batches in validation: [1,0]<stdout>: 19\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## num_train_batch on each GPU0 : 39 [1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## num_train_batch on each GPU5 : 39 [1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:1465 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 45.839909\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 43.585342\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 46.942684\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 45.686176\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 44.264427\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 44.274006\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 42.006134\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 44.046497\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 43.531288\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 44.238415\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 46.214149\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 43.619919\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 44.220028\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 44.964233\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 44.133072[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 45.316246\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:669:1457 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:678:1454 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 1, Test Loss: 2.2932920455932617, Test Accuracy: 12.849506378173828\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.349646\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.307919[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.360447\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.339609\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.308961\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.346674\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.346788\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.321728\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 2.330083\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 2.333593\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 2.347082\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 2.319762\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 2.349831\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 2.328220\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 2.313065\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 2.331002\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 2, Test Loss: 2.228142261505127, Test Accuracy: 16.817434310913086\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.204259\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.210730\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.249399\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.241187\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.241482\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.172848\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.185448\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.198061\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 2.192755\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 2.157443\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 2.183794\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 2.195693\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 2.206168\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 2.226160\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 2.180345\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 2.173409\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 3, Test Loss: 2.1646194458007812, Test Accuracy: 20.579771041870117[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.086732\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.183975\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.220032\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.152664\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.245223\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.171533\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.240407\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.185227\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 2.120680\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 2.194559\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 2.149211\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 2.178373\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 2.150662\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 2.120291\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 2.219109\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 2.187619\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 4, Test Loss: 2.089449882507324, Test Accuracy: 23.13939094543457\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.979374\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.969027\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.981454\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.006811\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.008089\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.032578\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.068012\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.986938\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 2.009119\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.975799\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 2.001479\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 2.037081\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 2.000236\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.974602\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.999527\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.983769\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 5, Test Loss: 2.0608248710632324, Test Accuracy: 24.393503189086914[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.136628\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.152581\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.084799\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 2.100378\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.119067\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.079105\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 2.085683\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.128334\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 2.162583\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 2.099816\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 2.095840\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 2.127154\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 2.128928\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 2.145758\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 2.155509\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 2.113157\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 6, Test Loss: 2.0249857902526855, Test Accuracy: 25.390625\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.020013\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.050246\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.035357\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.005520\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.986283\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.926065\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.057175\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.982495\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.923061\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.897954\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.989905\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 2.046316\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.954777\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.990960\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 2.021283\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.977968\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 7, Test Loss: 1.93934166431427, Test Accuracy: 29.31743621826172\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.035377\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.051547\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.081611\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.999087\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.991071\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.000966\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.983117\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.931333\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.933998\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.968261\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.923530\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.979462\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 2.029034\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 2.049215\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 2.034758\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.911745\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 8, Test Loss: 1.9617747068405151, Test Accuracy: 29.00904655456543\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.192038\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 2.020018\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.955818\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.906562\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 2.001324\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 2.041116\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.993436\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.973218\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 2.028690\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.990092\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 2.035829\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.985818\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 2.118753\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 2.007382\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 2.041574\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 2.061645\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 9, Test Loss: 1.9188705682754517, Test Accuracy: 30.160362243652344\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.839073\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.699414\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.765065\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.743294\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.822352\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.814817\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.831188\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.839593\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.857611\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.927654\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.800251\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.832536\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 2.014585\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.918193\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.868008\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.797777\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 10, Test Loss: 1.858498215675354, Test Accuracy: 31.476152420043945\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.839853\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.899976\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.887146\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.871306\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.924761\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.840770\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.825081\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.855433\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.865525\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.895833\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.860290\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.853348\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.895833\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.884316\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.808072\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.731495\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 11, Test Loss: 1.859017252922058, Test Accuracy: 32.689144134521484\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.903153\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.827846\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.963471\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.822337\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.929105\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.753518\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.885807\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.864776\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.904750\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 2.015070\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.943209\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.860258\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.952529\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.883009\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.852750\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.829074\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 12, Test Loss: 1.7994033098220825, Test Accuracy: 35.76274871826172\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.764720\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.794784\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.649619\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.743676\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.664514\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.603223\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.728435\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.720337\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.734097\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.703719\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.716586\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.647323\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.682001\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.630172\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.695078\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.599853\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 13, Test Loss: 1.7660819292068481, Test Accuracy: 35.690792083740234\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 2.083194\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.924928\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 2.035613\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.823593\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.926270\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.955554\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 2.062187\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.889693\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.897958\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.988777\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 2.050859\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.738231\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.916148\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.977754\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 2.016601\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 2.036379\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 14, Test Loss: 1.743919014930725, Test Accuracy: 36.379520416259766\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.654151\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.668401\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.771017\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.669306\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.614620\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.756653\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.577072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.651044\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.725126\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.554780\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.680734\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.765560\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.709464\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.596931\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.642842\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.678074\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 15, Test Loss: 1.838932991027832, Test Accuracy: 32.79193878173828\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.764912\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.753032\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.833420\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.759365\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.743423\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.765882\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.748775\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.746893\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.907114\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.854281\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.800203\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.777554\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.789164\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.842387\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.678321\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.706113\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 16, Test Loss: 1.7333890199661255, Test Accuracy: 36.55427551269531\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.933686\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.714017\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.779310\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.742447\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.775533\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.768480\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.663255\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.722139\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.728855\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.636390\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.790983\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.644175\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.821543\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.700338\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.727199\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.699447\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 17, Test Loss: 1.6931126117706299, Test Accuracy: 38.0859375\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.516123\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.454476\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.607099\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.446403\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.595742\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.594234\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.491600\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.595417\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.553966\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.463848\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.536535\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.602677\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.629687\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.659824\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.592008\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.502763\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 18, Test Loss: 1.702431082725525, Test Accuracy: 38.168174743652344\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.763875\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.759681\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.656237\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.726261\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.757860\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.806533\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.678478\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.791796\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.788912\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.661872\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.840188\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.706391\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.734707\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.838461\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.919004\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.628215\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 19, Test Loss: 1.664872169494629, Test Accuracy: 38.83634948730469\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.378682\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.513130\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.529053\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.595858\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.472700\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.562994\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.474820\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.523322\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.522532\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.402438\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.484460\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.539697\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.481569\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.452791\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.550570\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.523510\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 20, Test Loss: 1.6474651098251343, Test Accuracy: 40.86143112182617\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.639418\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.507773\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.701151\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.640999\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.595409\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.583609\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.667222\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.611266\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.671261\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.569252\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.627249\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.559892\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.586968\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.611916\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.622866\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.530730\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 21, Test Loss: 1.6452316045761108, Test Accuracy: 40.32688903808594\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.501162\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.564316\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.509504\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.476982\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.614338\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.467690\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.468540\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.534021\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.506539\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.476847\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.397919\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.530908\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.515542\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.524400\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.471675\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.595164\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 22, Test Loss: 1.6151189804077148, Test Accuracy: 41.40625\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.824179\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.652893\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.697019\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.650172\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.605875\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.651910\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.636697\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.681959\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.647024\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.801907\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.632272\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.523134\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.652245\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.559598\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.661298\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.631730\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 23, Test Loss: 1.6220191717147827, Test Accuracy: 41.37541198730469\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.583097\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.554330\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.695420\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.506326\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.536209\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.608249\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.596654\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.727327\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.681723\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.677508\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.571900\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.570667\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.503957\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.691497\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.443841\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.510934\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 24, Test Loss: 1.6810657978057861, Test Accuracy: 39.05221939086914\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.698009\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.713437\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.751998\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.709729\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.788019\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.713973\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.735436\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0[1,7]<stdout>:#011Loss: 1.614228\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.720342\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.681314\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.799915\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.657157\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.691873\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.709448\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.786941\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.865276\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 25, Test Loss: 1.6059612035751343, Test Accuracy: 41.96134948730469[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.528534\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.442390\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.521145\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.586349\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.561897\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.556373\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.542957\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.537731\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.516582\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.521762\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.590083\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.622888\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.622489\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.552043\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.605527\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.634891\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 26, Test Loss: 1.634045124053955, Test Accuracy: 41.03618621826172\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.476300\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.479212\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.518364\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.645756\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.547462\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.622366\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.481725\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.498028[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.514908\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.546532\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.490206\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.424710\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.621070\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.580447\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.449205\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.502324\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 27, Test Loss: 1.5691734552383423, Test Accuracy: 43.68832015991211\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.495258\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.403007\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.363466\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.379307\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.363549\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.363935\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.508303\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.448810\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.361362\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.505590\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.299818\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.378384\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.435387\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.391885\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.386993\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.484010\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 28, Test Loss: 1.7308467626571655, Test Accuracy: 38.89802551269531\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.839590\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.692863\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.795633\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.879225\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.825200\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.781097\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.844289\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.725756\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.777411\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.715868\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.778203\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.852152\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.915990\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.835864\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.863219\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.790404\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 29, Test Loss: 1.5838629007339478, Test Accuracy: 43.15378189086914\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.519191\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.404632\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.574081\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.504246\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.458983\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.427045\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.519498\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.480248\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.450166\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.517814\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.506140\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.460701\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.525719\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.515195\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.395395\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.515061\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 30, Test Loss: 1.5609078407287598, Test Accuracy: 43.29769515991211[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.332935\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.497836\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.532038\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.418116\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.583804\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.462197\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.505162\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.560414\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.566807\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.524251\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.641262\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.641414\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.483527\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.516269\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.368886\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.605625\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 31, Test Loss: 1.5532985925674438, Test Accuracy: 44.263980865478516\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.520648\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.471896\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.480098\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.555621\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.463971\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.657508\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.585021\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.588491\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.514520\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.550744\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.513976\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.544467\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.420960\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.422817\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.598803\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.481026\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 32, Test Loss: 1.5801012516021729, Test Accuracy: 43.71916198730469\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.441072\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.378735\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.333895\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.527643\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.381052\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.451801\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.298435\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.467859\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.525221\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.322431\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.382325\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.368084\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.491047\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.184043\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.377993\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.492760\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 33, Test Loss: 1.566704511642456, Test Accuracy: 43.75\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.348257\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.153920\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.398885\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.275753\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.329240\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.345312\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.254726\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.217169\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.243616\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.240882\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.319976\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.321837\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.267245\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.145490\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.333730\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.271563\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 34, Test Loss: 1.580668568611145, Test Accuracy: 43.49300765991211\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.429609\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.584537\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.475435\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.499211\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.562497\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.488130\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.537476\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.475571\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.440558\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.519461\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.590365\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.472853\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.318921\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.500368\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.350610\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.501789\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 35, Test Loss: 1.5397571325302124, Test Accuracy: 45.34333801269531\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.071959\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.317913\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.230347\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.251487\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.272609\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.216614\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.258913\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.295548\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.141388\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.335917\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.353179\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.169394\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.259986\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.180567\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.310315\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.339511\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 36, Test Loss: 1.4876782894134521, Test Accuracy: 47.11143112182617\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.673500\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.665744\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.479038\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.441874\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.530367\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.497872\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.587842\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.496559\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.621074\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.405452\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.586010\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.402833\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.507040\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.660615\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.538106\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.672713\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 37, Test Loss: 1.4821079969406128, Test Accuracy: 47.78988265991211\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.381592\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.474406\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.324642\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.327155\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.435691\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.444992[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.404006\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.379212\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.375352\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.406904\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.462302\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.488958\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.492233\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.441364\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.469768\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.520664\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 38, Test Loss: 1.5025135278701782, Test Accuracy: 46.361019134521484\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.625741\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.433024\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.727719\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.708473\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.525295\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.561512\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.553941\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.582765\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.422065\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.671246\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.546154\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.596296\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.663811\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.573056\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.622922\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.675926\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 39, Test Loss: 1.5018876791000366, Test Accuracy: 46.26850128173828\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.553388\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.552845\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.620633\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.546280\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.479816\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.488529\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.449094\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.493062\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.455257\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.557738\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.457061\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.630175\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.426769\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.469432\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.304518\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.541117\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 40, Test Loss: 1.4683438539505005, Test Accuracy: 46.74136734008789\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.337060\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.265386\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.271616\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.204854\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.320400\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.295924\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.212938\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.286291\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.272310\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.305922\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.279747\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.357652\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.297864\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.277241\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.246305\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.205405\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 41, Test Loss: 1.4974067211151123, Test Accuracy: 46.607730865478516[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.239461\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.363676\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.338900\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.277945\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.299646\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.397199\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.335924\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.249100\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.380498\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.309608\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.353211\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.363546\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.347355\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.429856\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.315624\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.297359\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 42, Test Loss: 1.456189751625061, Test Accuracy: 48.550575256347656[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.183627\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.209903\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.115427\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.166925\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.066567\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.045239\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.169032\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.130448\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.088316\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.122110\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.186498\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.024425\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.153332\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.214168\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.080055\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.124747\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 43, Test Loss: 1.4529284238815308, Test Accuracy: 49.167354583740234\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.261793\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.125806\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.121035\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.178096\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.282347\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.159399\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.000526\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.311483\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.095858\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.070172\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.113625\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.232560\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.236951\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.181676\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.139397\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.159866\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 44, Test Loss: 1.4242953062057495, Test Accuracy: 49.89720153808594\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.158644\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.182147\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.245021\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.215862\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.325443\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.176462\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.185174\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.307419\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.341313\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.123306\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.204859\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.277678\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.178976\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.319786\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.386050\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.279895\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 45, Test Loss: 1.4022890329360962, Test Accuracy: 50.13363265991211\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.373753\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.264665\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.293780\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.361925\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.289642\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.287966\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.323026\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.395392\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.282153\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.294195\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.296920\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.377801\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.311090\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.204241\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.228473\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.233800\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 46, Test Loss: 1.433719277381897, Test Accuracy: 50.04112243652344\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.385260\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.424966\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.326364\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.240111\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.338627\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.286153\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.276899\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.314758\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.203975\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.263325\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.274139\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.363636\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.268506\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.270187\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.288104\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.365111\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 47, Test Loss: 1.4337129592895508, Test Accuracy: 49.732730865478516\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.222909\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.332367\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.283973\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.279783\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.351781\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.299432\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.353369\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.301148\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.571791\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.402988\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.391384\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.421052\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.337404\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.269217\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.436834\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.322491\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 48, Test Loss: 1.4095920324325562, Test Accuracy: 50.42146301269531\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.136191\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.046071\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.348460\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.224502\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.098909\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.246811\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 1.206744\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0[1,6]<stdout>:#011Loss: 1.160488[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.158477\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.095195\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 1.102270\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.254871\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.203737\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.100163\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.169326\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.149791\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 49, Test Loss: 1.3828039169311523, Test Accuracy: 51.76809310913086\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## GPU0 - Step #0#011Loss: 1.021644\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:## GPU3 - Step #0#011Loss: 1.092821\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:## GPU2 - Step #0#011Loss: 1.096666\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:## GPU1 - Step #0#011Loss: 1.046594\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:## GPU5 - Step #0#011Loss: 1.086126\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:## GPU4 - Step #0#011Loss: 1.032556\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:## GPU7 - Step #0#011Loss: 0.973289\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:## GPU6 - Step #0#011Loss: 1.068302\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:## GPU9 - Step #0#011Loss: 1.078834\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:## GPU13 - Step #0#011Loss: 1.109374\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:## GPU15 - Step #0#011Loss: 1.287989\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:## GPU14 - Step #0#011Loss: 1.020976\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:## GPU12 - Step #0#011Loss: 1.059532\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:## GPU8 - Step #0#011Loss: 1.038304\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:## GPU10 - Step #0#011Loss: 0.999022\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:## GPU11 - Step #0#011Loss: 1.044193\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:## Epoch 50, Test Loss: 1.3374897241592407, Test Accuracy: 52.60074234008789\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training Finished.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Start Training  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:################# Saving Model  ################\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model is saved in /opt/ml/model\u001b[0m\n",
      "\u001b[35m2021-10-10 05:39:06,517 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.253.10' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-10-10 05:34:14.589555: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-10-10 05:34:14.589709: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-10-10 05:34:14.620838: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-10-10 05:34:14.621026: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-10-10 05:34:14.639815: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-10-10 05:34:14.639990: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-10-10 05:34:14.641548: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-10-10 05:34:14.662533: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-10-10 05:34:14.682265: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-10 05:34:14.693571: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-10 05:34:14.693722: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-10-10 05:34:14.694566: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-10-10 05:34:14.694697: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-10-10 05:34:14.702525: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-10-10 05:34:14.702691: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-10-10 05:34:14.714861: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-10-10 05:34:14.714840: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-10-10 05:34:14.714970: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-10-10 05:34:14.714993: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-10-10 05:34:14.716759: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-10-10 05:34:14.716827: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-10-10 05:34:14.716970: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-10-10 05:34:14.716968: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-10-10 05:34:14.717599: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-10-10 05:34:14.717759: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-10-10 05:34:14.718411: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-10-10 05:34:14.718537: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-10-10 05:34:14.719183: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-10-10 05:34:14.719309: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-10-10 05:34:14.760081: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-10-10 05:34:14.760083: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-10-10 05:34:14.760082: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-10-10 05:34:14.760372: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-10-10 05:34:14.761591: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-10-10 05:34:14.762039: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-10-10 05:34:14.763771: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-10-10 05:34:14.763571: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-10-10 05:34:14.768281: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-10-10 05:34:14.779350: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-10-10 05:34:14.787810: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-10-10 05:34:14.798776: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-10-10 05:34:14.824711: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-10-10 05:34:15.289509: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-10-10 05:34:15.289690: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-10-10 05:34:15.330868: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-10 05:34:15.346388: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-10 05:34:15.346548: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-10 05:34:15.387990: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-10-10 05:39:02.101411: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-10 05:39:06,503 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-10-10 05:39:47 Uploading - Uploading generated training model\n",
      "2021-10-10 05:39:47 Completed - Training job completed\n",
      "\u001b[35m2021-10-10 05:39:36,547 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2021-10-10 05:39:36,548 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[35mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[35mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[35m2021-10-10 05:39:36,548 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1042\n",
      "Billable seconds: 1042\n"
     ]
    }
   ],
   "source": [
    "ddp_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 정리 작업\n",
    "\n",
    "## 모델 아티펙트 저장\n",
    "- S3 에 저장된 모델 아티펙트를 저장하여 추론시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddp_artifact_path:  s3://sagemaker-us-east-1-057716757052/cifar10-sm-ddp-2021-10-10-05-27-44-881/output/model.tar.gz\n",
      "Stored 'tf2_ddp_artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "tf2_ddp_artifact_path = ddp_estimator.model_data\n",
    "print(\"ddp_artifact_path: \", tf2_ddp_artifact_path)\n",
    "\n",
    "\n",
    "%store tf2_ddp_artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-10 05:39:42    6052287 cifar10-sm-ddp-2021-10-10-05-27-44-881/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {tf2_ddp_artifact_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
